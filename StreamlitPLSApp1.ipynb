{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c06b4132-40ae-4f27-bfab-11597970f957",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app8.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app8.py\n",
    "#enhancing -enhanced the streamhandler and the chatbot --baselined for iqvia-x demo\n",
    "import os\n",
    "import re\n",
    "import urllib\n",
    "import urllib.request\n",
    "import base64\n",
    "import tempfile\n",
    "import json\n",
    "import nltk\n",
    "import streamlit as st\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches\n",
    "from pptx.dml.color import RGBColor\n",
    "from docx import Document\n",
    "from docx.shared import Pt\n",
    "from docx.enum.text import WD_PARAGRAPH_ALIGNMENT\n",
    "from datetime import datetime  # Import the 'datetime' class from the 'datetime' module\n",
    "import requests\n",
    "from Bio import Entrez\n",
    "from Bio import Medline\n",
    "import time\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory.chat_message_histories import StreamlitChatMessageHistory\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from shakti_stream_index import llama_vector_index\n",
    "from streamlit_pills import pills\n",
    "import streamlit_authenticator as stauth\n",
    "from streamlit_option_menu import option_menu\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.agents import create_json_agent, AgentExecutor\n",
    "from langchain.agents import create_pandas_dataframe_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from streamlit_chat import message\n",
    "from langchain.agents.agent_toolkits import JsonToolkit\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.requests import TextRequestsWrapper\n",
    "from langchain.tools.json.tool import JsonSpec\n",
    "from streamlit_image_select import image_select\n",
    "from streamlit_chat import message\n",
    "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index import download_loader, StorageContext, load_index_from_storage\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from langchain.callbacks.streamlit import StreamlitCallbackHandler\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#load inthe NTLK stopwords to remove articles, preposition and other words that are not actionable\n",
    "from nltk.corpus import stopwords\n",
    "# This allows to create individual objects from a bog of words\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Lemmatizer helps to reduce words to the base form\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Ngrams allows to group words in common pairs or trigrams..etc\n",
    "from nltk import ngrams\n",
    "# We can use counter to count the objects\n",
    "from collections import Counter\n",
    "# This is our word freq distribution library\n",
    "from nltk import FreqDist\n",
    "import seaborn as sns\n",
    "from time import sleep\n",
    "from stqdm import stqdm\n",
    "import itertools\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "saved_path = \"C:/Users/91961/maia/maia/bot_data\"\n",
    "rootdir = \"C:/Users/91961/maia/maia\"\n",
    "datadir = \"C:/Users/91961/maia/maia/data\"\n",
    "promptdir = \"C:/Users/91961/maia/maia/prompts\"\n",
    "Entrez.email = \"shakti20889@gmail.com\"\n",
    "\n",
    "\n",
    "# def progress_bar_method(secs):\n",
    "#     # Code for your second asynchronous method goes here\n",
    "#     for i in stqdm(range(secs), backend=True, frontend=True):\n",
    "#         sleep(0.5)\n",
    "\n",
    "# def generate_response1(input_text, df):\n",
    "#     agent = create_pandas_dataframe_agent(ChatOpenAI(temperature =0, model_name=\"gpt-4\", streaming = True), df, verbose=False)\n",
    "#     query_response = agent.run(input_text)\n",
    "#     return query_response\n",
    "\n",
    "\n",
    "def search_pubmed(article_title, retmax=5):\n",
    "    # Perform the PubMed search using the article title\n",
    "    handle = Entrez.esearch(db=\"pubmed\", term=article_title, retmax=retmax)\n",
    "    record = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    \n",
    "    # Retrieve the full study articles based on the search results\n",
    "    id_list = record[\"IdList\"]\n",
    "    handle = Entrez.efetch(db=\"pubmed\", id=id_list, rettype=\"medline\", retmode=\"text\")\n",
    "    records = Medline.parse(handle)\n",
    "    records = list(records)\n",
    "    handle.close()\n",
    "    \n",
    "    # Extract relevant information from the articles and return as JSON or CSV\n",
    "    articles = []\n",
    "    for record in records:\n",
    "        article = {\n",
    "            \"PMID\": record[\"PMID\"],\n",
    "            \"Title\": record[\"TI\"],\n",
    "            \"Abstract\": record.get(\"AB\", \"\"),\n",
    "            \"Citations\": f\"https://pubmed.ncbi.nlm.nih.gov/{record['PMID']}/\",\n",
    "        }\n",
    "        articles.append(article)\n",
    "    \n",
    "    # Return the articles as JSON or CSV\n",
    "    return articles\n",
    "\n",
    "def search_ctgov(article_title, retmax=5):\n",
    "    # Perform the ClinicalTrials.gov search using the article title\n",
    "    api_url = \"https://clinicaltrials.gov/api/query/full_studies\"\n",
    "    params = {\n",
    "        \"expr\": article_title,\n",
    "        \"min_rnk\": 1,\n",
    "        \"max_rnk\": retmax,\n",
    "        \"fmt\": \"json\",\n",
    "    }\n",
    "    response = requests.get(api_url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extract relevant information from the ctgov results and return as JSON or CSV\n",
    "    articles = []\n",
    "    for study in data.get(\"FullStudiesResponse\", {}).get(\"FullStudies\", []):\n",
    "        article = {\n",
    "            \"PMID\": study.get(\"Study\", {}).get(\"ProtocolSection\", {}).get(\"IdentificationModule\", {}).get(\"NCTId\", \"\"),\n",
    "            \"Title\": study.get(\"Study\", {}).get(\"ProtocolSection\", {}).get(\"IdentificationModule\", {}).get(\"OfficialTitle\", \"\"),\n",
    "            \"Abstract\": study.get(\"Study\", {}).get(\"ProtocolSection\", {}).get(\"DescriptionModule\", {}).get(\"BriefSummary\", \"\"),\n",
    "            \"Citations\": f\"https://clinicaltrials.gov/ct2/show/{study['Study']['ProtocolSection']['IdentificationModule']['NCTId']}\",\n",
    "        }\n",
    "        articles.append(article)\n",
    "    \n",
    "    # Return the articles as JSON or CSV\n",
    "    return articles\n",
    "\n",
    "# Function to display the article details in the main container\n",
    "def display_articles(articles):\n",
    "    for article in articles:\n",
    "        title = article[\"Title\"]\n",
    "        abstract = article[\"Abstract\"].strip().split(\". \", 3)[0] + \"...\"  # First 3 lines of abstract\n",
    "        citations_url = article[\"Citations\"]\n",
    "        st.write(f\"**Title:** {title}\")\n",
    "        st.write(f\"**Abstract:** {abstract}\")\n",
    "        st.write(f\"[Read More]({citations_url})\")\n",
    "        st.write(\"--------\")\n",
    "\n",
    "@st.cache_resource(ttl=\"1h\")\n",
    "def configure_retriever(uploaded_file):\n",
    "    # Read documents\n",
    "\n",
    "    temp_dir = tempfile.TemporaryDirectory()\n",
    "\n",
    "    temp_filepath = os.path.join(temp_dir.name, uploaded_file.name)\n",
    "    with open(temp_filepath, \"wb\") as f:\n",
    "        f.write(uploaded_file.getvalue())\n",
    "    loader = PyPDFLoader(temp_filepath)\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Split documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    # Create embeddings and store in vectordb\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vectordb = DocArrayInMemorySearch.from_documents(splits, embeddings)\n",
    "\n",
    "    # Define retriever\n",
    "    retriever = vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 2, \"fetch_k\": 4})\n",
    "\n",
    "    return retriever\n",
    "\n",
    "\n",
    "class StreamHandler(BaseCallbackHandler):\n",
    "    def __init__(self, container: st.delta_generator.DeltaGenerator, initial_text: str = \"\"):\n",
    "        self.container = container\n",
    "        self.text = initial_text\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        self.text += token\n",
    "        self.container.markdown(self.text)\n",
    "\n",
    "\n",
    "class PrintRetrievalHandler(BaseCallbackHandler):\n",
    "    def __init__(self, container):\n",
    "        self.container = container.expander(\"Context Retrieval\")\n",
    "\n",
    "    def on_retriever_start(self, query: str, **kwargs):\n",
    "        self.container.write(f\"**Question:** {query}\")\n",
    "\n",
    "    def on_retriever_end(self, documents, **kwargs):\n",
    "        # self.container.write(documents)\n",
    "        for idx, doc in enumerate(documents):\n",
    "            source = os.path.basename(doc.metadata[\"source\"])\n",
    "            self.container.write(f\"**Document {idx} from {source}**\")\n",
    "            self.container.markdown(doc.page_content)\n",
    "\n",
    "#function to set background image\n",
    "def set_bg_hack(main_bg):\n",
    "    '''\n",
    "    A function to unpack an image from root folder and set as bg.\n",
    " \n",
    "    Returns\n",
    "    -------\n",
    "    The background.\n",
    "    '''\n",
    "    # set bg name\n",
    "    main_bg_ext = \"png\"\n",
    "        \n",
    "    st.markdown(\n",
    "         f\"\"\"\n",
    "         <style>\n",
    "         .stApp {{\n",
    "             background: url(data:image/{main_bg_ext};base64,{base64.b64encode(open(main_bg, \"rb\").read()).decode()});\n",
    "             background-size: cover\n",
    "         }}\n",
    "         </style>\n",
    "         \"\"\",\n",
    "         unsafe_allow_html=True\n",
    "     )\n",
    "\n",
    "def sidebar_bg(side_bg):\n",
    "\n",
    "   side_bg_ext = 'png'\n",
    "\n",
    "   st.markdown(\n",
    "      f\"\"\"\n",
    "      <style>\n",
    "      [data-testid=\"stSidebar\"] > div:first-child {{\n",
    "          background: url(data:image/{side_bg_ext};base64,{base64.b64encode(open(side_bg, \"rb\").read()).decode()});\n",
    "      }}\n",
    "      </style>\n",
    "      \"\"\",\n",
    "      unsafe_allow_html=True,\n",
    "      )\n",
    "    \n",
    "def header_bg(side_bg):\n",
    "\n",
    "   side_bg_ext = 'png'\n",
    "\n",
    "   st.markdown(\n",
    "      f\"\"\"\n",
    "      <style>\n",
    "      header.css-1avcm0n {{\n",
    "          background: url(data:image/{side_bg_ext};base64,{base64.b64encode(open(side_bg, \"rb\").read()).decode()});\n",
    "      }}\n",
    "      </style>\n",
    "      \"\"\",\n",
    "      unsafe_allow_html=True,\n",
    "      )\n",
    "\n",
    "def get_sentiment(polarity):\n",
    "    if polarity < 0.0:\n",
    "        return 'Negative'\n",
    "    elif polarity > 0.2:\n",
    "        return 'Positive'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "    \n",
    "def word_frequency(sentence):\n",
    "    # joins all the sentenses\n",
    "    #sentence = \" \".join(sentence)\n",
    "    # creates tokens, creates lower class, removes numbers and lemmatizes the words\n",
    "    new_tokens = word_tokenize(sentence)\n",
    "    new_tokens = [t.lower() for t in new_tokens]\n",
    "    new_tokens =[t for t in new_tokens if t not in stopwords.words('english')]\n",
    "    new_tokens = [t for t in new_tokens if t.isalpha()]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    new_tokens =[lemmatizer.lemmatize(t) for t in new_tokens]\n",
    "    #counts the words, pairs and trigrams\n",
    "    counted = Counter(new_tokens)\n",
    "    counted_2= Counter(ngrams(new_tokens,2))\n",
    "    counted_3= Counter(ngrams(new_tokens,3))\n",
    "    #creates 3 data frames and returns thems\n",
    "    word_freq = pd.DataFrame(counted.items(),columns=['word','frequency']).sort_values(by='frequency',ascending=False)\n",
    "    word_pairs =pd.DataFrame(counted_2.items(),columns=['pairs','frequency']).sort_values(by='frequency',ascending=False)\n",
    "    trigrams =pd.DataFrame(counted_3.items(),columns=['trigrams','frequency']).sort_values(by='frequency',ascending=False)\n",
    "    return word_freq,word_pairs,trigrams    \n",
    "    \n",
    "#function to read prompt from corresponding text file\n",
    "def prompt(file):\n",
    "    with open(file,encoding=\"utf8\") as f:\n",
    "        return f.read()\n",
    "    \n",
    "#function to save a file\n",
    "def save_uploadedfile(uploaded_file):\n",
    "     with open(os.path.join(datadir, uploaded_file.name),\"wb\") as f:\n",
    "         f.write(uploaded_file.getbuffer())\n",
    "     return st.success(f\"\"\"Saved File:{uploaded_file.name} to directory\"\"\")\n",
    "\n",
    "# def create_vector():\n",
    "#     documents = SimpleDirectoryReader(saved_path).load_data()\n",
    "#     index = GPTVectorStoreIndex.from_documents(documents)\n",
    "\n",
    "#     storage_context = StorageContext.from_defaults()\n",
    "#     index.storage_context.persist(\"./vectordatabase\")\n",
    "#     #print (\"Done\")\n",
    "\n",
    "# def generate_response(prompt):\n",
    "#     storage_context = StorageContext.from_defaults(persist_dir=\"./vectordatabase\")\n",
    "#     index = load_index_from_storage(storage_context)\n",
    "#     query_engin = index.as_query_engine() \n",
    "#     question = prompt\n",
    "#     response = query_engin.query(question)\n",
    "#     return str(response)\n",
    "#     #print (\"\\n\", response)\n",
    "\n",
    "@st.cache_data\n",
    "#function to display the PDF of a given file \n",
    "def displayPDF(file):\n",
    "    # Opening file from file path\n",
    "    with open(file, \"rb\") as f:\n",
    "        base64_pdf = base64.b64encode(f.read()).decode('utf-8')\n",
    "\n",
    "    # Embedding PDF in HTML\n",
    "    pdf_display = F'<iframe src=\"data:application/pdf;base64,{base64_pdf}\" width=\"300\" height=\"1100\" type=\"application/pdf\"></iframe>'\n",
    "\n",
    "    # Displaying File\n",
    "    st.markdown(pdf_display, unsafe_allow_html=True)\n",
    "\n",
    "# Placeholder function for processing the uploaded documents\n",
    "def process_documents(NCT, uploaded_file, tense, pls_grade):\n",
    "    # Implement the document processing logic here\n",
    "\n",
    "    # Convert tense strings\n",
    "    tense_mapping = {\"on-going\": \"present\", \"completed\": \"past\", \"upcoming\": \"future\"}\n",
    "    tense = tense_mapping.get(tense, tense)\n",
    "    \n",
    "    summary_replacements = {\n",
    "        \"<Title>\": prompt(os.path.join(promptdir, 'title.txt')),\n",
    "        \"<Subtitle>\": prompt(os.path.join(promptdir, 'subtitle.txt')),\n",
    "        \"<Key takeaway>\": prompt(os.path.join(promptdir, 'key_takeaway.txt')),\n",
    "        \"<Phonetics>\": prompt(os.path.join(promptdir, 'phonetics.txt')), \n",
    "        \"<Introduction>\": prompt(os.path.join(promptdir, 'introduction.txt')), \n",
    "        \"<Intro summary>\": prompt(os.path.join(promptdir, 'intro_summary.txt')),\n",
    "        # \"<Inclusion criteria>\": \"\",\n",
    "        # \"<Exclusion crtieria>\": \"\",\n",
    "        # \"<Results>\": \"\",\n",
    "        \"<Aims>\": prompt(os.path.join(promptdir, 'aims.txt')),\n",
    "        \"<Conclusions>\": prompt(os.path.join(promptdir, 'conclusions.txt')),\n",
    "        # \"<Sponsor>\": \"\",\n",
    "        # \"<More Information>\": \"\",\n",
    "    }\n",
    "        \n",
    "    # Get the text for each section using GPTAPIcall function\n",
    "    for section_name, summary_prompt in summary_replacements.items():\n",
    "        \n",
    "        #prompt for pls grade and tense\n",
    "        query = f\"Strictly following the above instructions and the research document provided, write the content of {section_name} section of the plain language summary in {tense} tense.\\\n",
    "        Do not violate the section-wise instructions provided in any case. The content should be strictly inferred from the research document provided and not any other sources.\"\n",
    "        \n",
    "        st.subheader(f\"\"\":red[{section_name[1:-1]} :]\"\"\")\n",
    "        text = llama_vector_index(uploaded_file, prompt(os.path.join(promptdir, f'apls_persona_{pls_grade}_literacy.txt')) + \"\\n\" + summary_prompt + \"\\n\" + query)\n",
    "        summary_replacements[section_name] = str(text)\n",
    "        \n",
    "    ctgov_replacements = {\n",
    "                    \"<Start date>\": \"Answer the Study Start date in ```MMM-YYYY``` format\",\n",
    "                    \"<End date>\": \"Answer the Study End date in ```MMM-YYYY``` format\",\n",
    "                    \"<Participants>\": \"Total number of Participants in the study including drug arms, placebo arm, soc arm. Give one number answer\",\n",
    "                    \"<Arms count>\": \"Number of arms in the study including the drug arms, placebo arm, soc arm. Give one number answer\",\n",
    "                    \"<Disease condition>\": \"What is the disease condition for which drug is undergoing trials on patients in the study. Give answer as one disease\",\n",
    "                    \"<Demographics>\": \"What are the Demographics of participants in the study\",\n",
    "                    \"<treatment arm>\": \"Number of participants only in the drug arms of the study, do not count the participants from placebo arm or soc arm. Give one number answer\",\n",
    "                    \"<control arm>\": \"Number of participants in the placebo arm or soc arm. Give one number answer\",\n",
    "                    \"<Inclusion criteria>\": \"Inclusion criteria in EligibilityCriteria\",\n",
    "                    \"<Exclusion criteria>\": \"Exclusion criteria in EligibilityCriteria\",\n",
    "                    \"<Results>\": \"list all outcome measure results in bullets interms of outcome measure type, outcome measure title, outcome measure description, outcome measure value\",\n",
    "                    # \"<clinical trials gov link>\": \"https://clinicaltrials.gov/ct2/show/NCT03036813\",\n",
    "                    # \"<Summary date>\": datetime.now().strftime('%d-%b-%Y'),\n",
    "                    \"<Sponsor>\": \"Lead Sponsor Name\",\n",
    "                   }\n",
    "    \n",
    "    for section_name, ctgov_prompt in ctgov_replacements.items():\n",
    "        \n",
    "        st.subheader(f\"\"\":red[{section_name[1:-1]} :]\"\"\")\n",
    "        text = CTGovAPIcall(NCT, ctgov_prompt)\n",
    "        if section_name==\"<Participants>\":\n",
    "            text = re.findall(r'\\d+', text)\n",
    "        ctgov_replacements[section_name] = str(text)\n",
    "    \n",
    "    \n",
    "    replacements = {**summary_replacements, \n",
    "                    **ctgov_replacements, \n",
    "                    \"<Study number>\": f\"{NCT}\",\n",
    "                    \"<clinical trials gov link>\": f\"https://clinicaltrials.gov/ct2/show/{NCT}\",\n",
    "                    \"<Summary date>\": datetime.now().strftime('%d-%b-%Y'),\n",
    "                   }\n",
    "    \n",
    "    return replacements\n",
    "\n",
    "def CTGovAPIcall(NCT, query):\n",
    "    file_format = '&fmt=JSON'\n",
    "    expr = NCT #'A+Phase+3+Randomized+Trial+of+Voxelotor+in+Sickle+Cell+Disease' #or give NCT number here NCT03036813\n",
    "    ctgov = 'https://classic.clinicaltrials.gov/api/query/full_studies?expr='\n",
    "\n",
    "    your_url = (ctgov + expr + file_format)\n",
    "\n",
    "    with urllib.request.urlopen(your_url) as url:\n",
    "        ini_dict = json.loads(url.read().decode())\n",
    "        \n",
    "    json_spec = JsonSpec(dict_=ini_dict[\"FullStudiesResponse\"][\"FullStudies\"][0][\"Study\"], max_value_length=31000)\n",
    "    json_toolkit = JsonToolkit(spec=json_spec)\n",
    "    \n",
    "    # chat_box = st.empty()\n",
    "    # stream_handler = StreamHandler(chat_box, display_method='write')\n",
    "    \n",
    "    json_agent_executor = create_json_agent(\n",
    "        llm=ChatOpenAI(temperature=0, model_name=\"gpt-4-32k\", streaming=True, callbacks=[StreamlitCallbackHandler(st.container())],), toolkit=json_toolkit, verbose=True\n",
    "    )\n",
    "    resp = json_agent_executor.run(query)\n",
    "    st.write(resp)\n",
    "    return resp\n",
    "    \n",
    "# Placeholder function for postprocessing into PPT template\n",
    "def postprocess_to_ppt(replacements, selected_template):\n",
    "    # Implement the postprocessing logic here\n",
    "    # For demonstration purposes, we'll load a presentation object and copy the text from replacements dictionary\n",
    "    \n",
    "    #rootdir = os.path.realpath('./')\n",
    "    \n",
    "    #selected_template = \"PLS_PPT_Template\"\n",
    "    selected_template = selected_template[:-4]\n",
    "    ppt_file = f\"{selected_template}.pptx\"\n",
    "    prs = Presentation(os.path.join(rootdir, ppt_file))\n",
    "\n",
    "    for slide in prs.slides:\n",
    "        for shape in slide.shapes:\n",
    "            if shape.has_text_frame:\n",
    "                text_frame = shape.text_frame\n",
    "                for paragraph in text_frame.paragraphs:\n",
    "                    for run in paragraph.runs:\n",
    "                        for placeholder, new_text in replacements.items():\n",
    "                            if run.text == placeholder:\n",
    "                                # Preserve formatting of the first run in the paragraph\n",
    "                                first_run = paragraph.runs[0]\n",
    "                                font_size = first_run.font.size\n",
    "                                font_name = first_run.font.name\n",
    "                                font_bold = first_run.font.bold\n",
    "                                font_italic = first_run.font.italic\n",
    "\n",
    "                                # Check if font color is explicitly defined\n",
    "                                if first_run.font.color.type == \"rgb\":\n",
    "                                    font_color = first_run.font.color.rgb\n",
    "                                else:\n",
    "                                    font_color = None\n",
    "\n",
    "                                # Replace text while preserving formatting\n",
    "                                run.text = new_text\n",
    "\n",
    "                                # Apply preserved formatting to the entire paragraph\n",
    "                                for run in paragraph.runs:\n",
    "                                    run.font.size = font_size\n",
    "                                    run.font.name = font_name\n",
    "                                    run.font.bold = font_bold\n",
    "                                    run.font.italic = font_italic\n",
    "                                    if font_color:\n",
    "                                        run.font.color.rgb = font_color\n",
    "\n",
    "    # Return the modified presentation object\n",
    "    return prs\n",
    "\n",
    "\n",
    "# Placeholder function for postprocessing into DOC template\n",
    "def postprocess_to_doc(replacements):\n",
    "    \n",
    "    para_variable_list = [\"<Subtitle>\", \"<Key takeaway>\", \"<Phonetics>\", \"<Introduction>\", \"<Intro summary>\", \"<Demographics>\", \"<Inclusion criteria>\", \"<Exclusion criteria>\", \"<Results>\", \"<Aims>\", \"<Conclusions>\"]\n",
    "    table_variable_list = [\"<Study number>\", \"<Start date>\", \"<End date>\", \"<Participants>\", \"<Arms count>\", \"<treatment arm>\", \"<control arm>\", \"<Sponsor>\", \"<Summary date>\", \"<clinical trials gov link>\"]\n",
    "    # Create a new document\n",
    "    document = Document()\n",
    "\n",
    "    # Set the font size of the document\n",
    "    style = document.styles['Normal']\n",
    "    font = style.font\n",
    "    font.size = Pt(11)\n",
    "\n",
    "    # Set the title\n",
    "    title = replacements.get(\"<Title>\")\n",
    "    if title:\n",
    "        document.add_heading(title, level=1).bold = True\n",
    "\n",
    "    # Add paragraphs for para_variable_list with the same header formatting\n",
    "    for variable in para_variable_list:\n",
    "        value = replacements.get(variable)\n",
    "        if value:\n",
    "            p = document.add_paragraph(style='Heading 1')\n",
    "            p.text = variable[1:-1]\n",
    "            p.bold = True\n",
    "            document.add_paragraph(value)\n",
    "\n",
    "    # Add the table for table_variable_list\n",
    "    table_replacements = {variable: replacements.get(variable) for variable in table_variable_list}\n",
    "    if table_replacements:\n",
    "        table_heading = \"Additional Information\"\n",
    "        document.add_heading(table_heading, level=1)\n",
    "\n",
    "        # Create the table\n",
    "        table = document.add_table(rows=1, cols=2)\n",
    "        table.style = 'Table Grid'\n",
    "        \n",
    "        # Set table column widths\n",
    "        table.autofit = False\n",
    "        table.columns[0].width = Pt(200)\n",
    "        table.columns[1].width = Pt(300)\n",
    "\n",
    "        # Add table headers\n",
    "        table_header_cells = table.rows[0].cells\n",
    "        table_header_cells[0].text = \"Variable\"\n",
    "        table_header_cells[1].text = \"Value\"\n",
    "        for cell in table_header_cells:\n",
    "            cell.paragraphs[0].alignment = WD_PARAGRAPH_ALIGNMENT.CENTER\n",
    "            cell.paragraphs[0].bold = True\n",
    "\n",
    "        # Add table rows\n",
    "        for variable, value in table_replacements.items():\n",
    "            row_cells = table.add_row().cells\n",
    "            row_cells[0].text = variable[1:-1]\n",
    "            row_cells[1].text = value\n",
    "\n",
    "    return document\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    \n",
    "    #Page icons n tab name on browser tab\n",
    "    #img = Image.open(os.path.join(rootdir, 'pfizer.png'))\n",
    "    st.set_page_config(page_title = 'MAIA', page_icon = \":robot_face:\", layout=\"wide\")\n",
    "    \n",
    "    #to hide the hamburger running on top right and footer of streamlit\n",
    "    # hide_default_format = \"\"\"\n",
    "    #    <style>\n",
    "    #    #MainMenu {visibility: hidden; }\n",
    "    #    footer {visibility: hidden;}\n",
    "    #    </style>\n",
    "    #    \"\"\"\n",
    "    # st.markdown(hide_default_format, unsafe_allow_html=True)\n",
    "    \n",
    "    names = [\"admin\",\"shakti\"]\n",
    "    usernames = [\"adm\", \"shrp\"]\n",
    "    passwords = [\"abc123\", \"def456\"]\n",
    "\n",
    "    credentials = {\"usernames\":{}}\n",
    "    hashed_passwords = stauth.Hasher(passwords).generate()\n",
    "    \n",
    "    for uname, name, pwd in zip(usernames, names, hashed_passwords):\n",
    "        user_dict = {\"name\": name, \"password\": pwd}\n",
    "        credentials[\"usernames\"].update({uname: user_dict})\n",
    "\n",
    "    \n",
    "    #add a cookie which will be stored on client browser to save credentials till 30days\n",
    "    authenticator = stauth.Authenticate(credentials, \"pls_generator\", \"abcdef\", cookie_expiry_days = 30)\n",
    "\n",
    "    #u can locate the authenticator in the main body or the sidebar\n",
    "    name, authentication_status, username = authenticator.login(\"Login\", \"main\")\n",
    "    \n",
    "    if st.session_state[\"authentication_status\"] == False:\n",
    "        st.error(\"Username/password is incorrect\")\n",
    "        \n",
    "    if st.session_state[\"authentication_status\"] == None:\n",
    "        st.warning(\"Please enter your username and password\")\n",
    "        \n",
    "    if st.session_state[\"authentication_status\"]:\n",
    "        \n",
    "        #logout button on main container\n",
    "        authenticator.logout('Logout', 'main')\n",
    "        st.subheader(f'Welcome *{st.session_state[\"name\"]}*')\n",
    "        openai_api_key  = st.text_input(\"Enter your OpenAI API Key\", '',type=\"password\")\n",
    "        \n",
    "        #set bg image cover\n",
    "        #set_bg_hack(os.path.join(rootdir, 'iqvia-dark-blue.png'))\n",
    "        sidebar_bg(os.path.join(rootdir, 'iqvia-blue.png'))\n",
    "        #header_bg(os.path.join(rootdir, 'iqvia-dark-blue.png'))\n",
    "\n",
    "        #setting banner image\n",
    "        #st.image(Image.open(os.path.join(rootdir, 'Pfizer-AI.jpg')))\n",
    "        \n",
    "        selected_tab = option_menu(\n",
    "            menu_title=None,  # required\n",
    "            options=[\"PLS Generator\", \"Search your Document\", \"Word Analytics\", \"Converse with your Documents\", \"Search and Chat with PubMed/CTGov\"],  # required\n",
    "            icons=[\"house\", \"book\", \"envelope\", \"book\", \"envelope\"],  # optional\n",
    "            menu_icon=\"cast\",  # optional\n",
    "            default_index=0,  # optional\n",
    "            orientation=\"horizontal\",\n",
    "            # styles={\n",
    "            #     \"container\": {\"padding\": \"0!important\"},\n",
    "            #     \"icon\": {\"color\": \"orange\", \"font-size\": \"25px\"},\n",
    "            #     \"nav-link\": {\n",
    "            #         \"font-size\": \"25px\",\n",
    "            #         \"text-align\": \"left\",\n",
    "            #         \"margin\": \"0px\",\n",
    "            #         \"--hover-color\": \"#eee\",\n",
    "            #     },\n",
    "            #     \"nav-link-selected\": {\"background-color\": \"green\"},\n",
    "            # },\n",
    "        )\n",
    "        \n",
    "        #setting input components on sidebar\n",
    "        with st.sidebar:\n",
    "\n",
    "            st.image(Image.open(os.path.join(rootdir, 'iqvia-logo.png')))\n",
    "            #setting title\n",
    "            st.markdown(\"\"\"<h3 style='text-align: center'>*MAIA - Medical Affairs Intelligence Assistant*</h3>\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "            # Step 1: Document Upload\n",
    "            st.subheader(\"Step 1: Upload Clinical trial document\")\n",
    "            uploaded_file = st.file_uploader(\"Upload document\", accept_multiple_files=False, type=[\"pdf\"])\n",
    "            \n",
    "            NCT = st.text_input(\"Enter the NCT number:\", \"NCT\", key=\"NCT\")\n",
    "            \n",
    "            # Step 2: User Inputs\n",
    "            st.subheader(\"Step 2: Define the tone and Grade of PLS\")\n",
    "            # Set default values for radio button and slider\n",
    "            default_tense = \"Completed\"\n",
    "            default_pls_grade = \"Low\"\n",
    "\n",
    "            # Radio button for tense selection\n",
    "            tense = st.radio(\"Current status of the study for writing tense\", options=[\"On-going\", \"Completed\", \"Upcoming\"], key=\"tense\", index=[\"On-going\", \"Completed\", \"Upcoming\"].index(default_tense), horizontal=True)\n",
    "            #st.write('<style>div.row-widget.stRadio > div{flex-direction:row;}</style>', unsafe_allow_html=True)\n",
    "\n",
    "            # Slider for PLS grade selection\n",
    "            #pls_grade = st.slider(\"Health Literacy Grade Reading level\", min_value=0, max_value=10, step=5, key=\"pls_grade\", value=default_pls_grade)\n",
    "            pls_grade = st.select_slider(\"Health Literacy Grade of audience\", options=[\"Low\", \"High\"], key=\"pls_grade\", value = default_pls_grade)\n",
    "\n",
    "            st.session_state.process_button = False\n",
    "            process_button = st.button(\"Process Documents\")\n",
    "            st.session_state.process_button = process_button\n",
    "            st.session_state.uploaded_file = uploaded_file\n",
    "            st.session_state.selected_tab = selected_tab\n",
    "        \n",
    "        #if st.session_state.process_button and st.session_state.uploaded_file:\n",
    "        #if process_button and uploaded_file:\n",
    "            \n",
    "            # Retrieve user inputs if you haven't initialized them to any variable, then retrieve from streamlit session state\n",
    "            # st.session_state.tense = tense\n",
    "            # st.session_state.pls_grade = pls_grade\n",
    "            \n",
    "        if st.session_state.selected_tab == \"Search your Document\":\n",
    "            st.subheader(\"Ask your PDF üí¨\")\n",
    "            # show user input\n",
    "            user_question = st.text_input(\"Ask a question about your PDF:\", placeholder=\"Number of participants? \", disabled=not uploaded_file,)\n",
    "            \n",
    "            if st.session_state.uploaded_file:              \n",
    "                \n",
    "                # extract the text\n",
    "                if uploaded_file is not None:\n",
    "                  pdf_reader = PdfReader(uploaded_file)\n",
    "                  text = \"\"\n",
    "                  for page in pdf_reader.pages:\n",
    "                    text += page.extract_text()\n",
    "\n",
    "                  # split into chunks\n",
    "                  text_splitter = CharacterTextSplitter(\n",
    "                    separator=\"\\n\",\n",
    "                    chunk_size=1000,\n",
    "                    chunk_overlap=200,\n",
    "                    length_function=len\n",
    "                  )\n",
    "                  chunks = text_splitter.split_text(text)\n",
    "                    \n",
    "                  # create embeddings\n",
    "                  store_name = uploaded_file.name[:-4]\n",
    "                  if os.path.exists(os.path.join(datadir, f\"{store_name}.pkl\")):\n",
    "                    with open(os.path.join(datadir, f\"{store_name}.pkl\"), \"rb\") as f:\n",
    "                        knowledge_base = pickle.load(f)\n",
    "                        st.write('Embeddings loaded from the Disk:')\n",
    "                  else:\n",
    "                    embeddings = OpenAIEmbeddings()\n",
    "                    knowledge_base = FAISS.from_texts(chunks, embeddings)\n",
    "                    with open(os.path.join(datadir, f\"{store_name}.pkl\"), \"wb\") as f:\n",
    "                        pickle.dump(knowledge_base, f)\n",
    "                        st.write('Embeddings newly created')\n",
    "\n",
    "                  if user_question:\n",
    "                    docs = knowledge_base.similarity_search(user_question, k=3)\n",
    "\n",
    "                    # chat_box = st.empty()\n",
    "                    # stream_handler = StreamHandler(chat_box, display_method='write')\n",
    "        \n",
    "                    llm = ChatOpenAI(temperature=0, callbacks=[StreamlitCallbackHandler(st.container())], streaming=True)\n",
    "                    chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "                    \n",
    "                    #get_openai_callback() gives the cost on console\n",
    "                    # with get_openai_callback() as cb:\n",
    "                    #   response = chain.run(input_documents=docs, question=user_question)\n",
    "                    #   print(cb)\n",
    "                    response = chain.run(input_documents=docs, question=user_question)\n",
    "                    st.write(response)    \n",
    "        \n",
    "        if st.session_state.selected_tab == \"Converse with your Documents\":\n",
    "#             # Ensure the directory exists\n",
    "#             if not os.path.exists(saved_path):\n",
    "#                 os.makedirs(saved_path)\n",
    "        \n",
    "#             if uploaded_file is not None:\n",
    "#                 # To read file as bytes:\n",
    "#                 bytes_data = uploaded_file.getvalue()\n",
    "\n",
    "#                 # Save the uploaded file to the 'data' directory\n",
    "#                 with open(os.path.join(saved_path, uploaded_file.name), 'wb') as out_file:\n",
    "#                     out_file.write(bytes_data)\n",
    "\n",
    "#                 st.success('PDF file saved in data directory')\n",
    "#                 create_vector()\n",
    "#                 #remove_all_files(saved_path)\n",
    "#                 st.success('Vector created')\n",
    "\n",
    "#             # Initialise session state variables\n",
    "#             if 'generated' not in st.session_state:\n",
    "#                 st.session_state['generated'] = []\n",
    "#             if 'past' not in st.session_state:\n",
    "#                 st.session_state['past'] = []\n",
    "#             if 'messages' not in st.session_state:\n",
    "#                 st.session_state['messages'] = [\n",
    "#                     {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "#                 ]\n",
    "\n",
    "#             response_container = st.container()\n",
    "#             # container for text box\n",
    "#             container = st.container()\n",
    "\n",
    "#             with container:\n",
    "#                 with st.form(key='my_form', clear_on_submit=True):\n",
    "#                     user_input = st.text_area(\"You:\", key='input', height=50)\n",
    "#                     submit_button = st.form_submit_button(label='Send')\n",
    "\n",
    "#                 if submit_button and user_input:\n",
    "#                     output = generate_response(user_input)\n",
    "#                     st.session_state['past'].append(user_input)\n",
    "#                     st.session_state['generated'].append(output)\n",
    "#                     #st.session_state['model_name'].append(model_name)\n",
    "\n",
    "#             if st.session_state['generated']:\n",
    "#                 with response_container:\n",
    "#                     for i in range(len(st.session_state['generated'])):\n",
    "#                         message(st.session_state[\"past\"][i], is_user=True, key=str(i) + '_user1')\n",
    "#                         message(st.session_state[\"generated\"][i], key=str(i))\n",
    "            \n",
    "            if not uploaded_file:\n",
    "                st.caption(\"Please upload research PDF documents to continue.\")\n",
    "                st.stop()\n",
    "    \n",
    "            retriever = configure_retriever(uploaded_file)\n",
    "\n",
    "            # Setup memory for contextual conversation\n",
    "            msgs = StreamlitChatMessageHistory()\n",
    "            memory = ConversationBufferMemory(memory_key=\"chat_history\", chat_memory=msgs, return_messages=True)\n",
    "\n",
    "            # Setup LLM and QA chain\n",
    "            llm = ChatOpenAI(\n",
    "                model_name=\"gpt-3.5-turbo\", openai_api_key=openai_api_key, temperature=0, streaming=True\n",
    "            )\n",
    "            qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "                llm, retriever=retriever, memory=memory, verbose=True\n",
    "            )\n",
    "\n",
    "            if len(msgs.messages) == 0 or st.sidebar.button(\"Clear message history\"):\n",
    "                msgs.clear()\n",
    "                msgs.add_ai_message(\"How can I help you?\")\n",
    "\n",
    "            avatars = {\"human\": \"user\", \"ai\": \"assistant\"}\n",
    "            for msg in msgs.messages:\n",
    "                st.chat_message(avatars[msg.type]).write(msg.content)\n",
    "\n",
    "            if user_query := st.chat_input(placeholder=\"Ask me anything!\"):\n",
    "                st.chat_message(\"user\").write(user_query)\n",
    "\n",
    "                with st.chat_message(\"assistant\"):\n",
    "                    retrieval_handler = PrintRetrievalHandler(st.container())\n",
    "                    stream_handler = StreamHandler(st.empty())\n",
    "                    response = qa_chain.run(user_query, callbacks=[retrieval_handler, stream_handler])\n",
    "                \n",
    "    \n",
    "    \n",
    "        if st.session_state.selected_tab == \"Word Analytics\":    \n",
    "            #st.subheader(f\"You have selected {selected_tab}\")\n",
    "            \n",
    "            # extract the text\n",
    "            if uploaded_file is not None:\n",
    "                pdf_reader = PdfReader(uploaded_file)\n",
    "                text = \"\"\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text()\n",
    "                \n",
    "                #Remove un-important words:\n",
    "                stop_words = set(stopwords.words('english'))\n",
    "                query_words={'participants', 'Participants' }\n",
    "                stop_words.update(query_words)\n",
    "                for word in query_words:\n",
    "                    text = text.replace(word, '')\n",
    "                    \n",
    "                # Create and generate a word cloud image:\n",
    "                wordcloud = WordCloud(stopwords=stop_words, collocations=False, max_font_size=55, max_words=25, background_color=\"black\").generate(text)\n",
    "\n",
    "                # Display the generated image:\n",
    "                plt.figure(figsize=(10,12))\n",
    "                plt.imshow(wordcloud, interpolation='bilinear')\n",
    "                plt.axis(\"off\")\n",
    "                plt.show()\n",
    "                st.set_option('deprecation.showPyplotGlobalUse', False)\n",
    "                st.pyplot()\n",
    "                \n",
    "                col1, col2 = st.columns([0.3,0.7])\n",
    "                \n",
    "                with col1:\n",
    "                    #overall doc sentiment\n",
    "                    analyzer=SentimentIntensityAnalyzer()\n",
    "                    polarity = analyzer.polarity_scores(text)['compound']\n",
    "                    st.subheader(f\"\\nOverall Research document sentiment is: {get_sentiment(polarity)}\", )\n",
    "            \n",
    "                    # splitting modified single string into list of strings using groupby() function\n",
    "                    grouped_strings = [\"\".join(g) for k, g in itertools.groupby(text, lambda x: x == \" \") if not k]\n",
    "                    \n",
    "                    #word-wise sentiment\n",
    "                    df = pd.DataFrame()\n",
    "                    df['polarity']=[analyzer.polarity_scores(text)['compound'] for text in grouped_strings]\n",
    "                    df['sentiment']=df.polarity.apply(get_sentiment)\n",
    "                    plt.figure(figsize=(3,3))\n",
    "                    df.sentiment.value_counts().plot.bar()\n",
    "                    st.pyplot()\n",
    "                    \n",
    "                    #Tokenization\n",
    "                    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "                    #compute freq distribution\n",
    "                    freq_dist = FreqDist(tokens)\n",
    "\n",
    "                    #plot the freq distribution\n",
    "                    freq_dist.plot(50, cumulative=True)\n",
    "\n",
    "                    #set labels and title\n",
    "                    # plt.xlabel('Words')\n",
    "                    # plt.ylabel('Frequency')\n",
    "                    plt.title('Frequency distribution of words')\n",
    "                    st.pyplot()\n",
    "                    \n",
    "                with col2:\n",
    "                    st.subheader(\"\\nWord frequency of the words in the research doc is: \")\n",
    "                    \n",
    "                    data2,data3,data4 = word_frequency(text)\n",
    "                    fig, axes = plt.subplots(3,1,figsize=(8,20))\n",
    "                    sns.barplot(ax=axes[0],x='frequency',y='word',data=data2.head(30))\n",
    "                    sns.barplot(ax=axes[1],x='frequency',y='pairs',data=data3.head(30))\n",
    "                    sns.barplot(ax=axes[2],x='frequency',y='trigrams',data=data4.head(30))\n",
    "                    st.pyplot(fig)\n",
    "                \n",
    "        if st.session_state.selected_tab == \"PLS Generator\":\n",
    "            if st.session_state.process_button and st.session_state.uploaded_file and st.session_state.NCT!='NCT':\n",
    "                col1, col2 = st.columns([0.2,0.8], gap=\"large\")\n",
    "                with col1:\n",
    "                    input_file = save_uploadedfile(uploaded_file)\n",
    "                    pdf_file = os.path.join(datadir, uploaded_file.name) #rootdir + \"/\" + uploaded_file.name\n",
    "                    pdf_view = displayPDF(pdf_file)\n",
    "                with col2:\n",
    "                    with st.spinner(text='Processing research document you gave on the left to generate Plain Language Summary for you...‚è≥'):\n",
    "\n",
    "                        # Progress bar\n",
    "                        #progress_bar_method(50) or st.progress(0, \"text\")\n",
    "                        \n",
    "                        # Call the processing function on the uploaded documents with user inputs\n",
    "                        replacements = process_documents(NCT, pdf_file, tense, pls_grade)\n",
    "                        st.success(\"Processed Output to be filled up in the preferred PLS template\")\n",
    "\n",
    "                        #Display processed output\n",
    "                        #st.write(replacements)\n",
    "                        st.snow()\n",
    "                        st.balloons()\n",
    "                        \n",
    "                # Store the replacements dictionary in session state\n",
    "                st.session_state.replacements = replacements\n",
    "\n",
    "            # Step 3: PPT Template Selection and Download\n",
    "            st.subheader(\"Step 3: Select PLS Template and Download\")\n",
    "            \n",
    "            default_format = \"PPT format\"\n",
    "            st.session_state.select_format = pills(\"Select PPT or Word format\", [\"PPT format\", \"Word format\"], [\"üéà\", \"üåà\"], index=[\"PPT format\", \"Word format\"].index(default_format))\n",
    "            \n",
    "            if st.session_state.select_format == \"PPT format\":\n",
    "                # Add radio buttons for template selection here    \n",
    "                default_template = \"Blue_PLS_Template.png\"\n",
    "                selected_template = image_select(\n",
    "                    label=\"Select PPT Template\",\n",
    "                    images=[\n",
    "                        os.path.join(rootdir, 'Pfizer_Blue_PLS_Template.png'),\n",
    "                        os.path.join(rootdir, 'Pfizer_Red_PLS_Template.png'),\n",
    "                        os.path.join(rootdir, 'Pfizer_Long_PLS_Template.png'),\n",
    "                    ],\n",
    "                    captions=[\"Blue_PLS_Template\", \"Red_PLS_Template\", \"Long_PLS_Template\"],\n",
    "                    index=[\"Blue_PLS_Template.png\", \"Red_PLS_Template.png\", \"Long_PLS_Template.png\"].index(default_template),\n",
    "                    use_container_width = False,\n",
    "                )\n",
    "                #selected_template = st.radio(\"Select PPT Template\", options=[\"Blue_PLS_Template\", \"Red_PLS_Template\", \"Long_PLS_Template\"], index=[\"Blue_PLS_Template\", \"Red_PLS_Template\", \"Long_PLS_Template\"].index(default_template), horizontal=True)\n",
    "                #selected_template = pills(\"\", [\"Pfizer_Blue_PLS_Template\", \"Pfizer_Red_PLS_Template\", \"Pfizer_Long_PLS_Template\"], [\"üçÄ\", \"üéà\", \"üåà\"])\n",
    "            \n",
    "            if st.session_state.select_format == \"Word format\":\n",
    "                default_template = \"Word_PLS_Template\"\n",
    "                selected_template = \"Blue_PLS_Template.png\"\n",
    "                image_select(\n",
    "                    label=\"Select Word Template\",\n",
    "                    images=[\n",
    "                        os.path.join(rootdir, 'Pfizer_Word_PLS_Template.png'),\n",
    "                    ],\n",
    "                    captions=[\"Word_PLS_Template\"],\n",
    "                    index=[\"Word_PLS_Template\"].index(default_template),\n",
    "                    use_container_width = False,\n",
    "                )\n",
    "            \n",
    "            generate_ppt_button = st.button(\"Generate PLS\")\n",
    "\n",
    "            if generate_ppt_button:\n",
    "                # Retrieve the replacements dictionary from session state\n",
    "                replacements = st.session_state.replacements\n",
    "                st.session_state.process_button = False\n",
    "\n",
    "                if replacements:\n",
    "                    with st.spinner('Generating PLS slides for you...‚è≥'):\n",
    "                        # Call the postprocessing function to generate PPT content\n",
    "                        ppt_content = postprocess_to_ppt(replacements, selected_template)\n",
    "\n",
    "                        doc_content = postprocess_to_doc(replacements)\n",
    "\n",
    "                        # Display the PPT content using st.markdown or st.write\n",
    "                        #st.markdown(ppt_content, unsafe_allow_html=True)\n",
    "                        st.markdown(list(replacements.keys()))\n",
    "\n",
    "                        # Store the PPT content in session state\n",
    "                        st.session_state.ppt_content = ppt_content\n",
    "                        st.session_state.doc_content = doc_content\n",
    "\n",
    "                         # Step 4: PPT Download\n",
    "                        if \"ppt_content\" and \"doc_content\" in st.session_state:\n",
    "                            ppt_content = st.session_state.ppt_content\n",
    "                            doc_content = st.session_state.doc_content\n",
    "\n",
    "                            st.session_state.replacements = replacements\n",
    "                            st.session_state.process_button = False\n",
    "\n",
    "                            # Save the modified presentation object to a temporary file\n",
    "                            #ppt_output_file = f\"PLS_{replacements['<Title>']}_{datetime.now().strftime('%Y%m%d%H%M%S')}.pptx\"                    \n",
    "                            ppt_output_file = \"PLS_PPT.pptx\"\n",
    "                            #ppt_content.save(ppt_output_file)\n",
    "\n",
    "                            # save presentation as binary output\n",
    "                            binary_output = BytesIO()\n",
    "                            ppt_content.save(binary_output)\n",
    "\n",
    "                            binary_output_doc = BytesIO()\n",
    "                            doc_content.save(binary_output_doc)\n",
    "\n",
    "                            # display success message and download button\n",
    "                            st.success(':tada: The PLS template has been filled with above sections in ' + selected_template)\n",
    "\n",
    "                            # Provide the download link for the generated PPT and DOC\n",
    "                            if st.session_state.select_format == \"PPT format\":\n",
    "                                st.download_button(\"Download PLS PPT\", data=binary_output.getvalue(), file_name=ppt_output_file, mime=\"application/vnd.openxmlformats-officedocument.presentationml.presentation\")\n",
    "                            if st.session_state.select_format == \"Word format\":\n",
    "                                st.download_button(\"Download PLS Doc\", data=binary_output_doc.getvalue(), file_name=\"PLS_DOC.docx\", mime=\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\")\n",
    "\n",
    "                                \n",
    "        if st.session_state.selected_tab == \"Search and Chat with PubMed/CTGov\":\n",
    "            st.subheader(\"Search PubMed and ClinicalTrials.gov\")\n",
    "            query = st.text_input(\"Enter your query:\")\n",
    "            search_option = st.radio(\"Search Option\", [\"Search PubMed\", \"Search ClinicalTrials.gov\", \"Search Both\"], horizontal=True)\n",
    "\n",
    "            col1, col2 = st.columns(2)  # Split the main container into two columns\n",
    "            if query:\n",
    "                if search_option == \"Search PubMed\":\n",
    "                    pubmed_articles = search_pubmed(query)[:5]\n",
    "                    st.session_state.articles = pubmed_articles\n",
    "                    with col1:\n",
    "                        st.write(\"### PubMed Results\")\n",
    "                        display_articles(pubmed_articles)\n",
    "                    # Create a DataFrame in the second column with PubMed articles\n",
    "                    with col2:\n",
    "                        df_pubmed = pd.DataFrame(pubmed_articles)\n",
    "                        st.write(\"### PubMed DataFrame\")\n",
    "                        st.write(df_pubmed)\n",
    "\n",
    "                elif search_option == \"Search ClinicalTrials.gov\":\n",
    "                    ctgov_articles = search_ctgov(query)[:5]\n",
    "                    st.session_state.articles = ctgov_articles\n",
    "                    with col1:\n",
    "                        st.write(\"### ClinicalTrials.gov Results\")\n",
    "                        display_articles(ctgov_articles)\n",
    "                    # Create a DataFrame in the second column with ClinicalTrials.gov articles\n",
    "                    with col2:\n",
    "                        df_ctgov = pd.DataFrame(ctgov_articles)\n",
    "                        st.write(\"### ClinicalTrials.gov DataFrame\")\n",
    "                        st.write(df_ctgov)\n",
    "\n",
    "                else:  # Search Both\n",
    "                    pubmed_articles = search_pubmed(query)[:5]\n",
    "                    ctgov_articles = search_ctgov(query)[:5]\n",
    "                    st.session_state.articles = pubmed_articles + ctgov_articles\n",
    "                    with col1:\n",
    "                        st.write(\"### PubMed Results\")\n",
    "                        display_articles(pubmed_articles)\n",
    "                    with col1:  # Use the same column for ClinicalTrials.gov Results\n",
    "                        st.write(\"### ClinicalTrials.gov Results\")\n",
    "                        display_articles(ctgov_articles)\n",
    "                    # Concatenate both sets of articles and create a DataFrame in the second column\n",
    "                    with col2:\n",
    "                        combined_articles = pubmed_articles + ctgov_articles\n",
    "                        df_combined = pd.DataFrame(combined_articles)\n",
    "                        st.write(\"### Combined DataFrame\")\n",
    "                        st.write(df_combined)\n",
    "            # Placeholder for chatbot implementation in the second column\n",
    "            with col2:\n",
    "                st.write(\"Chatbot - Ask questions from only these Pubmed/CTGov articles\")\n",
    "                if 'articles' in st.session_state:      \n",
    "                    df = pd.DataFrame(st.session_state.articles)\n",
    "                    #user_prompt = st.text_area(label=\"prompt:\",placeholder=\"Number of patients..\",)\n",
    "                    #if st.button(\"Generate\"):\n",
    "                ########################################################LangChain CSV Agent (with Pandas)\n",
    "\n",
    "                    # langchain_pandas_agent = create_pandas_dataframe_agent(\n",
    "                    #     ChatOpenAI(temperature=0, model=\"gpt-4-32k\", streaming=True, ),\n",
    "                    #     df,\n",
    "                    #     verbose=True,\n",
    "                    #     agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    "                    # )\n",
    "\n",
    "                    #st.write(\"Langchain pandas agent: \", langchain_pandas_agent.run(user_prompt))\n",
    "\n",
    "                    # Initialise session state variables\n",
    "                    if 'generated1' not in st.session_state:\n",
    "                        st.session_state['generated1'] = []\n",
    "                    if 'past1' not in st.session_state:\n",
    "                        st.session_state['past1'] = []\n",
    "                    if 'messages1' not in st.session_state:\n",
    "                        st.session_state['messages1'] = [\n",
    "                            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "                        ]\n",
    "\n",
    "\n",
    "                    # container for chat history\n",
    "                    response_container = st.container()\n",
    "\n",
    "                    # container for text box\n",
    "                    input_container = st.container()\n",
    "\n",
    "                    with input_container:\n",
    "                        # Create a form for user input\n",
    "                        with st.form(key='my_form', clear_on_submit=True):\n",
    "                            user_input = st.text_area(\"You:\", key='input', height=100)\n",
    "                            submit_button = st.form_submit_button(label='Send')\n",
    "\n",
    "                        if submit_button and user_input:\n",
    "                            # If user submits input, generate response and store input and response in session state variables\n",
    "                            try:\n",
    "                                query_response = generate_response1(user_input, df)\n",
    "                                st.session_state['past1'].append(user_input)\n",
    "                                st.session_state['generated1'].append(query_response)\n",
    "                            except Exception as e:\n",
    "                                st.error(\"An error occurred: {}\".format(e))\n",
    "\n",
    "                    if st.session_state['generated1']:\n",
    "                        # Display chat history in a container\n",
    "                        with response_container:\n",
    "                            for i in range(len(st.session_state['generated1'])):\n",
    "                                message(st.session_state[\"past1\"][i], is_user=True, key=str(i) + '_user')\n",
    "                                message(st.session_state[\"generated1\"][i], key=str(i))\n",
    "                        \n",
    "                        \n",
    "                        # Add a download button for the chat conversation\n",
    "                        #if st.button(\"Download Chat Conversation\"):\n",
    "                            #download_chat_conversation(st.session_state['past'], st.session_state['generated'])\n",
    "                                \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f808e624-62e3-4ed3-ae83-daf7d5e783b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app8.py\n",
    "#enhancing -current working version, modifying the pubmed chatbot with pandas df agent\n",
    "import os\n",
    "import re\n",
    "import urllib\n",
    "import urllib.request\n",
    "import base64\n",
    "import tempfile\n",
    "import json\n",
    "import nltk\n",
    "import streamlit as st\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches\n",
    "from pptx.dml.color import RGBColor\n",
    "from docx import Document\n",
    "from docx.shared import Pt\n",
    "from docx.enum.text import WD_PARAGRAPH_ALIGNMENT\n",
    "from datetime import datetime  # Import the 'datetime' class from the 'datetime' module\n",
    "import requests\n",
    "from Bio import Entrez\n",
    "from Bio import Medline\n",
    "import time\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory.chat_message_histories import StreamlitChatMessageHistory\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from shakti_stream_index import llama_vector_index\n",
    "from streamlit_pills import pills\n",
    "import streamlit_authenticator as stauth\n",
    "from streamlit_option_menu import option_menu\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.agents import create_json_agent, AgentExecutor\n",
    "from langchain.agents import create_pandas_dataframe_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from streamlit_chat import message\n",
    "from langchain.agents.agent_toolkits import JsonToolkit\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.requests import TextRequestsWrapper\n",
    "from langchain.tools.json.tool import JsonSpec\n",
    "from streamlit_image_select import image_select\n",
    "from streamlit_chat import message\n",
    "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index import download_loader, StorageContext, load_index_from_storage\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from langchain.callbacks.streamlit import StreamlitCallbackHandler\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#load inthe NTLK stopwords to remove articles, preposition and other words that are not actionable\n",
    "from nltk.corpus import stopwords\n",
    "# This allows to create individual objects from a bog of words\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Lemmatizer helps to reduce words to the base form\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Ngrams allows to group words in common pairs or trigrams..etc\n",
    "from nltk import ngrams\n",
    "# We can use counter to count the objects\n",
    "from collections import Counter\n",
    "# This is our word freq distribution library\n",
    "from nltk import FreqDist\n",
    "import seaborn as sns\n",
    "from time import sleep\n",
    "from stqdm import stqdm\n",
    "import itertools\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "saved_path = \"C:/Users/91961/maia/maia/bot_data\"\n",
    "rootdir = \"C:/Users/91961/maia/maia\"\n",
    "datadir = \"C:/Users/91961/maia/maia/data\"\n",
    "promptdir = \"C:/Users/91961/maia/maia/prompts\"\n",
    "Entrez.email = \"shakti20889@gmail.com\"\n",
    "\n",
    "\n",
    "# def progress_bar_method(secs):\n",
    "#     # Code for your second asynchronous method goes here\n",
    "#     for i in stqdm(range(secs), backend=True, frontend=True):\n",
    "#         sleep(0.5)\n",
    "\n",
    "def generate_response1(input_text, df):\n",
    "    agent = create_pandas_dataframe_agent(ChatOpenAI(temperature =0, model_name=\"gpt-4\", streaming = True), df, verbose=False)\n",
    "    query_response = agent.run(input_text)\n",
    "    return query_response\n",
    "\n",
    "\n",
    "def search_pubmed(article_title, retmax=5):\n",
    "    # Perform the PubMed search using the article title\n",
    "    handle = Entrez.esearch(db=\"pubmed\", term=article_title, retmax=retmax)\n",
    "    record = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    \n",
    "    # Retrieve the full study articles based on the search results\n",
    "    id_list = record[\"IdList\"]\n",
    "    handle = Entrez.efetch(db=\"pubmed\", id=id_list, rettype=\"medline\", retmode=\"text\")\n",
    "    records = Medline.parse(handle)\n",
    "    records = list(records)\n",
    "    handle.close()\n",
    "    \n",
    "    # Extract relevant information from the articles and return as JSON or CSV\n",
    "    articles = []\n",
    "    for record in records:\n",
    "        article = {\n",
    "            \"PMID\": record[\"PMID\"],\n",
    "            \"Title\": record[\"TI\"],\n",
    "            \"Abstract\": record.get(\"AB\", \"\"),\n",
    "            \"Citations\": f\"https://pubmed.ncbi.nlm.nih.gov/{record['PMID']}/\",\n",
    "        }\n",
    "        articles.append(article)\n",
    "    \n",
    "    # Return the articles as JSON or CSV\n",
    "    return articles\n",
    "\n",
    "def search_ctgov(article_title, retmax=5):\n",
    "    # Perform the ClinicalTrials.gov search using the article title\n",
    "    api_url = \"https://clinicaltrials.gov/api/query/full_studies\"\n",
    "    params = {\n",
    "        \"expr\": article_title,\n",
    "        \"min_rnk\": 1,\n",
    "        \"max_rnk\": retmax,\n",
    "        \"fmt\": \"json\",\n",
    "    }\n",
    "    response = requests.get(api_url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extract relevant information from the ctgov results and return as JSON or CSV\n",
    "    articles = []\n",
    "    for study in data.get(\"FullStudiesResponse\", {}).get(\"FullStudies\", []):\n",
    "        article = {\n",
    "            \"PMID\": study.get(\"Study\", {}).get(\"ProtocolSection\", {}).get(\"IdentificationModule\", {}).get(\"NCTId\", \"\"),\n",
    "            \"Title\": study.get(\"Study\", {}).get(\"ProtocolSection\", {}).get(\"IdentificationModule\", {}).get(\"OfficialTitle\", \"\"),\n",
    "            \"Abstract\": study.get(\"Study\", {}).get(\"ProtocolSection\", {}).get(\"DescriptionModule\", {}).get(\"BriefSummary\", \"\"),\n",
    "            \"Citations\": f\"https://clinicaltrials.gov/ct2/show/{study['Study']['ProtocolSection']['IdentificationModule']['NCTId']}\",\n",
    "        }\n",
    "        articles.append(article)\n",
    "    \n",
    "    # Return the articles as JSON or CSV\n",
    "    return articles\n",
    "\n",
    "# Function to display the article details in the main container\n",
    "def display_articles(articles):\n",
    "    for article in articles:\n",
    "        title = article[\"Title\"]\n",
    "        abstract = article[\"Abstract\"].strip().split(\". \", 3)[0] + \"...\"  # First 3 lines of abstract\n",
    "        citations_url = article[\"Citations\"]\n",
    "        st.write(f\"**Title:** {title}\")\n",
    "        st.write(f\"**Abstract:** {abstract}\")\n",
    "        st.write(f\"[Read More]({citations_url})\")\n",
    "        st.write(\"--------\")\n",
    "\n",
    "@st.cache_resource(ttl=\"1h\")\n",
    "def configure_retriever(uploaded_file):\n",
    "    # Read documents\n",
    "\n",
    "    temp_dir = tempfile.TemporaryDirectory()\n",
    "\n",
    "    temp_filepath = os.path.join(temp_dir.name, uploaded_file.name)\n",
    "    with open(temp_filepath, \"wb\") as f:\n",
    "        f.write(uploaded_file.getvalue())\n",
    "    loader = PyPDFLoader(temp_filepath)\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Split documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    # Create embeddings and store in vectordb\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vectordb = DocArrayInMemorySearch.from_documents(splits, embeddings)\n",
    "\n",
    "    # Define retriever\n",
    "    retriever = vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 2, \"fetch_k\": 4})\n",
    "\n",
    "    return retriever\n",
    "\n",
    "\n",
    "class StreamHandler(BaseCallbackHandler):\n",
    "    def __init__(self, container: st.delta_generator.DeltaGenerator, initial_text: str = \"\"):\n",
    "        self.container = container\n",
    "        self.text = initial_text\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        self.text += token\n",
    "        self.container.markdown(self.text)\n",
    "\n",
    "\n",
    "class PrintRetrievalHandler(BaseCallbackHandler):\n",
    "    def __init__(self, container):\n",
    "        self.container = container.expander(\"Context Retrieval\")\n",
    "\n",
    "    def on_retriever_start(self, query: str, **kwargs):\n",
    "        self.container.write(f\"**Question:** {query}\")\n",
    "\n",
    "    def on_retriever_end(self, documents, **kwargs):\n",
    "        # self.container.write(documents)\n",
    "        for idx, doc in enumerate(documents):\n",
    "            source = os.path.basename(doc.metadata[\"source\"])\n",
    "            self.container.write(f\"**Document {idx} from {source}**\")\n",
    "            self.container.markdown(doc.page_content)\n",
    "\n",
    "#function to set background image\n",
    "def set_bg_hack(main_bg):\n",
    "    '''\n",
    "    A function to unpack an image from root folder and set as bg.\n",
    " \n",
    "    Returns\n",
    "    -------\n",
    "    The background.\n",
    "    '''\n",
    "    # set bg name\n",
    "    main_bg_ext = \"png\"\n",
    "        \n",
    "    st.markdown(\n",
    "         f\"\"\"\n",
    "         <style>\n",
    "         .stApp {{\n",
    "             background: url(data:image/{main_bg_ext};base64,{base64.b64encode(open(main_bg, \"rb\").read()).decode()});\n",
    "             background-size: cover\n",
    "         }}\n",
    "         </style>\n",
    "         \"\"\",\n",
    "         unsafe_allow_html=True\n",
    "     )\n",
    "\n",
    "def sidebar_bg(side_bg):\n",
    "\n",
    "   side_bg_ext = 'png'\n",
    "\n",
    "   st.markdown(\n",
    "      f\"\"\"\n",
    "      <style>\n",
    "      [data-testid=\"stSidebar\"] > div:first-child {{\n",
    "          background: url(data:image/{side_bg_ext};base64,{base64.b64encode(open(side_bg, \"rb\").read()).decode()});\n",
    "      }}\n",
    "      </style>\n",
    "      \"\"\",\n",
    "      unsafe_allow_html=True,\n",
    "      )\n",
    "    \n",
    "def header_bg(side_bg):\n",
    "\n",
    "   side_bg_ext = 'png'\n",
    "\n",
    "   st.markdown(\n",
    "      f\"\"\"\n",
    "      <style>\n",
    "      header.css-1avcm0n {{\n",
    "          background: url(data:image/{side_bg_ext};base64,{base64.b64encode(open(side_bg, \"rb\").read()).decode()});\n",
    "      }}\n",
    "      </style>\n",
    "      \"\"\",\n",
    "      unsafe_allow_html=True,\n",
    "      )\n",
    "\n",
    "def get_sentiment(polarity):\n",
    "    if polarity < 0.0:\n",
    "        return 'Negative'\n",
    "    elif polarity > 0.2:\n",
    "        return 'Positive'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "    \n",
    "def word_frequency(sentence):\n",
    "    # joins all the sentenses\n",
    "    #sentence = \" \".join(sentence)\n",
    "    # creates tokens, creates lower class, removes numbers and lemmatizes the words\n",
    "    new_tokens = word_tokenize(sentence)\n",
    "    new_tokens = [t.lower() for t in new_tokens]\n",
    "    new_tokens =[t for t in new_tokens if t not in stopwords.words('english')]\n",
    "    new_tokens = [t for t in new_tokens if t.isalpha()]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    new_tokens =[lemmatizer.lemmatize(t) for t in new_tokens]\n",
    "    #counts the words, pairs and trigrams\n",
    "    counted = Counter(new_tokens)\n",
    "    counted_2= Counter(ngrams(new_tokens,2))\n",
    "    counted_3= Counter(ngrams(new_tokens,3))\n",
    "    #creates 3 data frames and returns thems\n",
    "    word_freq = pd.DataFrame(counted.items(),columns=['word','frequency']).sort_values(by='frequency',ascending=False)\n",
    "    word_pairs =pd.DataFrame(counted_2.items(),columns=['pairs','frequency']).sort_values(by='frequency',ascending=False)\n",
    "    trigrams =pd.DataFrame(counted_3.items(),columns=['trigrams','frequency']).sort_values(by='frequency',ascending=False)\n",
    "    return word_freq,word_pairs,trigrams    \n",
    "    \n",
    "#function to read prompt from corresponding text file\n",
    "def prompt(file):\n",
    "    with open(file,encoding=\"utf8\") as f:\n",
    "        return f.read()\n",
    "    \n",
    "#function to save a file\n",
    "def save_uploadedfile(uploaded_file):\n",
    "     with open(os.path.join(datadir, uploaded_file.name),\"wb\") as f:\n",
    "         f.write(uploaded_file.getbuffer())\n",
    "     return st.success(f\"\"\"Saved File:{uploaded_file.name} to directory\"\"\")\n",
    "\n",
    "# def create_vector():\n",
    "#     documents = SimpleDirectoryReader(saved_path).load_data()\n",
    "#     index = GPTVectorStoreIndex.from_documents(documents)\n",
    "\n",
    "#     storage_context = StorageContext.from_defaults()\n",
    "#     index.storage_context.persist(\"./vectordatabase\")\n",
    "#     #print (\"Done\")\n",
    "\n",
    "# def generate_response(prompt):\n",
    "#     storage_context = StorageContext.from_defaults(persist_dir=\"./vectordatabase\")\n",
    "#     index = load_index_from_storage(storage_context)\n",
    "#     query_engin = index.as_query_engine() \n",
    "#     question = prompt\n",
    "#     response = query_engin.query(question)\n",
    "#     return str(response)\n",
    "#     #print (\"\\n\", response)\n",
    "\n",
    "@st.cache_data\n",
    "#function to display the PDF of a given file \n",
    "def displayPDF(file):\n",
    "    # Opening file from file path\n",
    "    with open(file, \"rb\") as f:\n",
    "        base64_pdf = base64.b64encode(f.read()).decode('utf-8')\n",
    "\n",
    "    # Embedding PDF in HTML\n",
    "    pdf_display = F'<iframe src=\"data:application/pdf;base64,{base64_pdf}\" width=\"300\" height=\"1100\" type=\"application/pdf\"></iframe>'\n",
    "\n",
    "    # Displaying File\n",
    "    st.markdown(pdf_display, unsafe_allow_html=True)\n",
    "\n",
    "# Placeholder function for processing the uploaded documents\n",
    "def process_documents(NCT, uploaded_file, tense, pls_grade):\n",
    "    # Implement the document processing logic here\n",
    "\n",
    "    # Convert tense strings\n",
    "    tense_mapping = {\"on-going\": \"present\", \"completed\": \"past\", \"upcoming\": \"future\"}\n",
    "    tense = tense_mapping.get(tense, tense)\n",
    "    \n",
    "    summary_replacements = {\n",
    "        \"<Title>\": prompt(os.path.join(promptdir, 'title.txt')),\n",
    "        \"<Subtitle>\": prompt(os.path.join(promptdir, 'subtitle.txt')),\n",
    "        \"<Key takeaway>\": prompt(os.path.join(promptdir, 'key_takeaway.txt')),\n",
    "        \"<Phonetics>\": prompt(os.path.join(promptdir, 'phonetics.txt')), \n",
    "        \"<Introduction>\": prompt(os.path.join(promptdir, 'introduction.txt')), \n",
    "        \"<Intro summary>\": prompt(os.path.join(promptdir, 'intro_summary.txt')),\n",
    "        # \"<Inclusion criteria>\": \"\",\n",
    "        # \"<Exclusion crtieria>\": \"\",\n",
    "        # \"<Results>\": \"\",\n",
    "        \"<Aims>\": prompt(os.path.join(promptdir, 'aims.txt')),\n",
    "        \"<Conclusions>\": prompt(os.path.join(promptdir, 'conclusions.txt')),\n",
    "        # \"<Sponsor>\": \"\",\n",
    "        # \"<More Information>\": \"\",\n",
    "    }\n",
    "        \n",
    "    # Get the text for each section using GPTAPIcall function\n",
    "    for section_name, summary_prompt in summary_replacements.items():\n",
    "        \n",
    "        #prompt for pls grade and tense\n",
    "        query = f\"Strictly following the above instructions and the research document provided, write the content of {section_name} section of the plain language summary in {tense} tense.\\\n",
    "        Do not violate the section-wise instructions provided in any case. The content should be strictly inferred from the research document provided and not any other sources.\"\n",
    "        \n",
    "        st.subheader(f\"\"\":red[{section_name[1:-1]} :]\"\"\")\n",
    "        text = llama_vector_index(uploaded_file, prompt(os.path.join(promptdir, f'apls_persona_{pls_grade}_literacy.txt')) + \"\\n\" + summary_prompt + \"\\n\" + query)\n",
    "        summary_replacements[section_name] = str(text)\n",
    "        \n",
    "    ctgov_replacements = {\n",
    "                    \"<Start date>\": \"Answer the Study Start date in ```MMM-YYYY``` format\",\n",
    "                    \"<End date>\": \"Answer the Study End date in ```MMM-YYYY``` format\",\n",
    "                    \"<Participants>\": \"Total number of Participants in the study including drug arms, placebo arm, soc arm. Give one number answer\",\n",
    "                    \"<Arms count>\": \"Number of arms in the study including the drug arms, placebo arm, soc arm. Give one number answer\",\n",
    "                    \"<Disease condition>\": \"What is the disease condition for which drug is undergoing trials on patients in the study. Give answer as one disease\",\n",
    "                    \"<Demographics>\": \"What are the Demographics of participants in the study\",\n",
    "                    \"<treatment arm>\": \"Number of participants only in the drug arms of the study, do not count the participants from placebo arm or soc arm. Give one number answer\",\n",
    "                    \"<control arm>\": \"Number of participants in the placebo arm or soc arm. Give one number answer\",\n",
    "                    \"<Inclusion criteria>\": \"Inclusion criteria in EligibilityCriteria\",\n",
    "                    \"<Exclusion criteria>\": \"Exclusion criteria in EligibilityCriteria\",\n",
    "                    \"<Results>\": \"list all outcome measure results in bullets interms of outcome measure type, outcome measure title, outcome measure description, outcome measure value\",\n",
    "                    # \"<clinical trials gov link>\": \"https://clinicaltrials.gov/ct2/show/NCT03036813\",\n",
    "                    # \"<Summary date>\": datetime.now().strftime('%d-%b-%Y'),\n",
    "                    \"<Sponsor>\": \"Lead Sponsor Name\",\n",
    "                   }\n",
    "    \n",
    "    for section_name, ctgov_prompt in ctgov_replacements.items():\n",
    "        \n",
    "        st.subheader(f\"\"\":red[{section_name[1:-1]} :]\"\"\")\n",
    "        text = CTGovAPIcall(NCT, ctgov_prompt)\n",
    "        if section_name==\"<Participants>\":\n",
    "            text = re.findall(r'\\d+', text)\n",
    "        ctgov_replacements[section_name] = str(text)\n",
    "    \n",
    "    \n",
    "    replacements = {**summary_replacements, \n",
    "                    **ctgov_replacements, \n",
    "                    \"<Study number>\": f\"{NCT}\",\n",
    "                    \"<clinical trials gov link>\": f\"https://clinicaltrials.gov/ct2/show/{NCT}\",\n",
    "                    \"<Summary date>\": datetime.now().strftime('%d-%b-%Y'),\n",
    "                   }\n",
    "    \n",
    "    return replacements\n",
    "\n",
    "def CTGovAPIcall(NCT, query):\n",
    "    file_format = '&fmt=JSON'\n",
    "    expr = NCT #'A+Phase+3+Randomized+Trial+of+Voxelotor+in+Sickle+Cell+Disease' #or give NCT number here NCT03036813\n",
    "    ctgov = 'https://classic.clinicaltrials.gov/api/query/full_studies?expr='\n",
    "\n",
    "    your_url = (ctgov + expr + file_format)\n",
    "\n",
    "    with urllib.request.urlopen(your_url) as url:\n",
    "        ini_dict = json.loads(url.read().decode())\n",
    "        \n",
    "    json_spec = JsonSpec(dict_=ini_dict[\"FullStudiesResponse\"][\"FullStudies\"][0][\"Study\"], max_value_length=31000)\n",
    "    json_toolkit = JsonToolkit(spec=json_spec)\n",
    "    \n",
    "    # chat_box = st.empty()\n",
    "    # stream_handler = StreamHandler(chat_box, display_method='write')\n",
    "    \n",
    "    json_agent_executor = create_json_agent(\n",
    "        llm=ChatOpenAI(temperature=0, model_name=\"gpt-4-32k\", streaming=True, callbacks=[StreamlitCallbackHandler(st.container())],), toolkit=json_toolkit, verbose=True\n",
    "    )\n",
    "    resp = json_agent_executor.run(query)\n",
    "    st.write(resp)\n",
    "    return resp\n",
    "    \n",
    "# Placeholder function for postprocessing into PPT template\n",
    "def postprocess_to_ppt(replacements, selected_template):\n",
    "    # Implement the postprocessing logic here\n",
    "    # For demonstration purposes, we'll load a presentation object and copy the text from replacements dictionary\n",
    "    \n",
    "    #rootdir = os.path.realpath('./')\n",
    "    \n",
    "    #selected_template = \"PLS_PPT_Template\"\n",
    "    selected_template = selected_template[:-4]\n",
    "    ppt_file = f\"{selected_template}.pptx\"\n",
    "    prs = Presentation(os.path.join(rootdir, ppt_file))\n",
    "\n",
    "    for slide in prs.slides:\n",
    "        for shape in slide.shapes:\n",
    "            if shape.has_text_frame:\n",
    "                text_frame = shape.text_frame\n",
    "                for paragraph in text_frame.paragraphs:\n",
    "                    for run in paragraph.runs:\n",
    "                        for placeholder, new_text in replacements.items():\n",
    "                            if run.text == placeholder:\n",
    "                                # Preserve formatting of the first run in the paragraph\n",
    "                                first_run = paragraph.runs[0]\n",
    "                                font_size = first_run.font.size\n",
    "                                font_name = first_run.font.name\n",
    "                                font_bold = first_run.font.bold\n",
    "                                font_italic = first_run.font.italic\n",
    "\n",
    "                                # Check if font color is explicitly defined\n",
    "                                if first_run.font.color.type == \"rgb\":\n",
    "                                    font_color = first_run.font.color.rgb\n",
    "                                else:\n",
    "                                    font_color = None\n",
    "\n",
    "                                # Replace text while preserving formatting\n",
    "                                run.text = new_text\n",
    "\n",
    "                                # Apply preserved formatting to the entire paragraph\n",
    "                                for run in paragraph.runs:\n",
    "                                    run.font.size = font_size\n",
    "                                    run.font.name = font_name\n",
    "                                    run.font.bold = font_bold\n",
    "                                    run.font.italic = font_italic\n",
    "                                    if font_color:\n",
    "                                        run.font.color.rgb = font_color\n",
    "\n",
    "    # Return the modified presentation object\n",
    "    return prs\n",
    "\n",
    "\n",
    "# Placeholder function for postprocessing into DOC template\n",
    "def postprocess_to_doc(replacements):\n",
    "    \n",
    "    para_variable_list = [\"<Subtitle>\", \"<Key takeaway>\", \"<Phonetics>\", \"<Introduction>\", \"<Intro summary>\", \"<Demographics>\", \"<Inclusion criteria>\", \"<Exclusion criteria>\", \"<Results>\", \"<Aims>\", \"<Conclusions>\"]\n",
    "    table_variable_list = [\"<Study number>\", \"<Start date>\", \"<End date>\", \"<Participants>\", \"<Arms count>\", \"<treatment arm>\", \"<control arm>\", \"<Sponsor>\", \"<Summary date>\", \"<clinical trials gov link>\"]\n",
    "    # Create a new document\n",
    "    document = Document()\n",
    "\n",
    "    # Set the font size of the document\n",
    "    style = document.styles['Normal']\n",
    "    font = style.font\n",
    "    font.size = Pt(11)\n",
    "\n",
    "    # Set the title\n",
    "    title = replacements.get(\"<Title>\")\n",
    "    if title:\n",
    "        document.add_heading(title, level=1).bold = True\n",
    "\n",
    "    # Add paragraphs for para_variable_list with the same header formatting\n",
    "    for variable in para_variable_list:\n",
    "        value = replacements.get(variable)\n",
    "        if value:\n",
    "            p = document.add_paragraph(style='Heading 1')\n",
    "            p.text = variable[1:-1]\n",
    "            p.bold = True\n",
    "            document.add_paragraph(value)\n",
    "\n",
    "    # Add the table for table_variable_list\n",
    "    table_replacements = {variable: replacements.get(variable) for variable in table_variable_list}\n",
    "    if table_replacements:\n",
    "        table_heading = \"Additional Information\"\n",
    "        document.add_heading(table_heading, level=1)\n",
    "\n",
    "        # Create the table\n",
    "        table = document.add_table(rows=1, cols=2)\n",
    "        table.style = 'Table Grid'\n",
    "        \n",
    "        # Set table column widths\n",
    "        table.autofit = False\n",
    "        table.columns[0].width = Pt(200)\n",
    "        table.columns[1].width = Pt(300)\n",
    "\n",
    "        # Add table headers\n",
    "        table_header_cells = table.rows[0].cells\n",
    "        table_header_cells[0].text = \"Variable\"\n",
    "        table_header_cells[1].text = \"Value\"\n",
    "        for cell in table_header_cells:\n",
    "            cell.paragraphs[0].alignment = WD_PARAGRAPH_ALIGNMENT.CENTER\n",
    "            cell.paragraphs[0].bold = True\n",
    "\n",
    "        # Add table rows\n",
    "        for variable, value in table_replacements.items():\n",
    "            row_cells = table.add_row().cells\n",
    "            row_cells[0].text = variable[1:-1]\n",
    "            row_cells[1].text = value\n",
    "\n",
    "    return document\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    \n",
    "    #Page icons n tab name on browser tab\n",
    "    #img = Image.open(os.path.join(rootdir, 'pfizer.png'))\n",
    "    st.set_page_config(page_title = 'MAIA', page_icon = \":robot_face:\", layout=\"wide\")\n",
    "    \n",
    "    #to hide the hamburger running on top right and footer of streamlit\n",
    "    # hide_default_format = \"\"\"\n",
    "    #    <style>\n",
    "    #    #MainMenu {visibility: hidden; }\n",
    "    #    footer {visibility: hidden;}\n",
    "    #    </style>\n",
    "    #    \"\"\"\n",
    "    # st.markdown(hide_default_format, unsafe_allow_html=True)\n",
    "    \n",
    "    names = [\"admin\",\"shakti\"]\n",
    "    usernames = [\"adm\", \"shrp\"]\n",
    "    passwords = [\"abc123\", \"def456\"]\n",
    "\n",
    "    credentials = {\"usernames\":{}}\n",
    "    hashed_passwords = stauth.Hasher(passwords).generate()\n",
    "    \n",
    "    for uname, name, pwd in zip(usernames, names, hashed_passwords):\n",
    "        user_dict = {\"name\": name, \"password\": pwd}\n",
    "        credentials[\"usernames\"].update({uname: user_dict})\n",
    "\n",
    "    \n",
    "    #add a cookie which will be stored on client browser to save credentials till 30days\n",
    "    authenticator = stauth.Authenticate(credentials, \"pls_generator\", \"abcdef\", cookie_expiry_days = 30)\n",
    "\n",
    "    #u can locate the authenticator in the main body or the sidebar\n",
    "    name, authentication_status, username = authenticator.login(\"Login\", \"main\")\n",
    "    \n",
    "    if st.session_state[\"authentication_status\"] == False:\n",
    "        st.error(\"Username/password is incorrect\")\n",
    "        \n",
    "    if st.session_state[\"authentication_status\"] == None:\n",
    "        st.warning(\"Please enter your username and password\")\n",
    "        \n",
    "    if st.session_state[\"authentication_status\"]:\n",
    "        \n",
    "        #logout button on main container\n",
    "        authenticator.logout('Logout', 'main')\n",
    "        st.subheader(f'Welcome *{st.session_state[\"name\"]}*')\n",
    "        openai_api_key  = st.text_input(\"Enter your OpenAI API Key\", '',type=\"password\")\n",
    "        \n",
    "        #set bg image cover\n",
    "        #set_bg_hack(os.path.join(rootdir, 'iqvia-dark-blue.png'))\n",
    "        sidebar_bg(os.path.join(rootdir, 'iqvia-blue.png'))\n",
    "        #header_bg(os.path.join(rootdir, 'iqvia-dark-blue.png'))\n",
    "\n",
    "        #setting banner image\n",
    "        #st.image(Image.open(os.path.join(rootdir, 'Pfizer-AI.jpg')))\n",
    "        \n",
    "        selected_tab = option_menu(\n",
    "            menu_title=None,  # required\n",
    "            options=[\"PLS Generator\", \"Search your Document\", \"Word Analytics\", \"Converse with your Documents\", \"Search and Chat with PubMed/CTGov\"],  # required\n",
    "            icons=[\"house\", \"book\", \"envelope\", \"book\", \"envelope\"],  # optional\n",
    "            menu_icon=\"cast\",  # optional\n",
    "            default_index=0,  # optional\n",
    "            orientation=\"horizontal\",\n",
    "            # styles={\n",
    "            #     \"container\": {\"padding\": \"0!important\"},\n",
    "            #     \"icon\": {\"color\": \"orange\", \"font-size\": \"25px\"},\n",
    "            #     \"nav-link\": {\n",
    "            #         \"font-size\": \"25px\",\n",
    "            #         \"text-align\": \"left\",\n",
    "            #         \"margin\": \"0px\",\n",
    "            #         \"--hover-color\": \"#eee\",\n",
    "            #     },\n",
    "            #     \"nav-link-selected\": {\"background-color\": \"green\"},\n",
    "            # },\n",
    "        )\n",
    "        \n",
    "        #setting input components on sidebar\n",
    "        with st.sidebar:\n",
    "\n",
    "            st.image(Image.open(os.path.join(rootdir, 'iqvia-logo.png')))\n",
    "            #setting title\n",
    "            st.markdown(\"\"\"<h3 style='text-align: center'>*MAIA - Medical Affairs Intelligence Assistant*</h3>\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "            # Step 1: Document Upload\n",
    "            st.subheader(\"Step 1: Upload Clinical trial document\")\n",
    "            uploaded_file = st.file_uploader(\"Upload document\", accept_multiple_files=False, type=[\"pdf\"])\n",
    "            \n",
    "            NCT = st.text_input(\"Enter the NCT number:\", \"NCT\", key=\"NCT\")\n",
    "            \n",
    "            # Step 2: User Inputs\n",
    "            st.subheader(\"Step 2: Define the tone and Grade of PLS\")\n",
    "            # Set default values for radio button and slider\n",
    "            default_tense = \"Completed\"\n",
    "            default_pls_grade = \"Low\"\n",
    "\n",
    "            # Radio button for tense selection\n",
    "            tense = st.radio(\"Current status of the study for writing tense\", options=[\"On-going\", \"Completed\", \"Upcoming\"], key=\"tense\", index=[\"On-going\", \"Completed\", \"Upcoming\"].index(default_tense), horizontal=True)\n",
    "            #st.write('<style>div.row-widget.stRadio > div{flex-direction:row;}</style>', unsafe_allow_html=True)\n",
    "\n",
    "            # Slider for PLS grade selection\n",
    "            #pls_grade = st.slider(\"Health Literacy Grade Reading level\", min_value=0, max_value=10, step=5, key=\"pls_grade\", value=default_pls_grade)\n",
    "            pls_grade = st.select_slider(\"Health Literacy Grade of audience\", options=[\"Low\", \"High\"], key=\"pls_grade\", value = default_pls_grade)\n",
    "\n",
    "            st.session_state.process_button = False\n",
    "            process_button = st.button(\"Process Documents\")\n",
    "            st.session_state.process_button = process_button\n",
    "            st.session_state.uploaded_file = uploaded_file\n",
    "            st.session_state.selected_tab = selected_tab\n",
    "        \n",
    "        #if st.session_state.process_button and st.session_state.uploaded_file:\n",
    "        #if process_button and uploaded_file:\n",
    "            \n",
    "            # Retrieve user inputs if you haven't initialized them to any variable, then retrieve from streamlit session state\n",
    "            # st.session_state.tense = tense\n",
    "            # st.session_state.pls_grade = pls_grade\n",
    "            \n",
    "        if st.session_state.selected_tab == \"Search your Document\":\n",
    "            st.subheader(\"Ask your PDF üí¨\")\n",
    "            # show user input\n",
    "            user_question = st.text_input(\"Ask a question about your PDF:\", placeholder=\"Number of participants? \", disabled=not uploaded_file,)\n",
    "            \n",
    "            if st.session_state.uploaded_file:              \n",
    "                \n",
    "                # extract the text\n",
    "                if uploaded_file is not None:\n",
    "                  pdf_reader = PdfReader(uploaded_file)\n",
    "                  text = \"\"\n",
    "                  for page in pdf_reader.pages:\n",
    "                    text += page.extract_text()\n",
    "\n",
    "                  # split into chunks\n",
    "                  text_splitter = CharacterTextSplitter(\n",
    "                    separator=\"\\n\",\n",
    "                    chunk_size=1000,\n",
    "                    chunk_overlap=200,\n",
    "                    length_function=len\n",
    "                  )\n",
    "                  chunks = text_splitter.split_text(text)\n",
    "                    \n",
    "                  # create embeddings\n",
    "                  store_name = uploaded_file.name[:-4]\n",
    "                  if os.path.exists(os.path.join(datadir, f\"{store_name}.pkl\")):\n",
    "                    with open(os.path.join(datadir, f\"{store_name}.pkl\"), \"rb\") as f:\n",
    "                        knowledge_base = pickle.load(f)\n",
    "                        st.write('Embeddings loaded from the Disk:')\n",
    "                  else:\n",
    "                    embeddings = OpenAIEmbeddings()\n",
    "                    knowledge_base = FAISS.from_texts(chunks, embeddings)\n",
    "                    with open(os.path.join(datadir, f\"{store_name}.pkl\"), \"wb\") as f:\n",
    "                        pickle.dump(knowledge_base, f)\n",
    "                        st.write('Embeddings newly created')\n",
    "\n",
    "                  if user_question:\n",
    "                    docs = knowledge_base.similarity_search(user_question, k=3)\n",
    "\n",
    "                    # chat_box = st.empty()\n",
    "                    # stream_handler = StreamHandler(chat_box, display_method='write')\n",
    "        \n",
    "                    llm = ChatOpenAI(temperature=0, callbacks=[StreamlitCallbackHandler(st.container())], streaming=True)\n",
    "                    chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "                    \n",
    "                    #get_openai_callback() gives the cost on console\n",
    "                    # with get_openai_callback() as cb:\n",
    "                    #   response = chain.run(input_documents=docs, question=user_question)\n",
    "                    #   print(cb)\n",
    "                    response = chain.run(input_documents=docs, question=user_question)\n",
    "                    st.write(response)    \n",
    "        \n",
    "        if st.session_state.selected_tab == \"Converse with your Documents\":\n",
    "#             # Ensure the directory exists\n",
    "#             if not os.path.exists(saved_path):\n",
    "#                 os.makedirs(saved_path)\n",
    "        \n",
    "#             if uploaded_file is not None:\n",
    "#                 # To read file as bytes:\n",
    "#                 bytes_data = uploaded_file.getvalue()\n",
    "\n",
    "#                 # Save the uploaded file to the 'data' directory\n",
    "#                 with open(os.path.join(saved_path, uploaded_file.name), 'wb') as out_file:\n",
    "#                     out_file.write(bytes_data)\n",
    "\n",
    "#                 st.success('PDF file saved in data directory')\n",
    "#                 create_vector()\n",
    "#                 #remove_all_files(saved_path)\n",
    "#                 st.success('Vector created')\n",
    "\n",
    "#             # Initialise session state variables\n",
    "#             if 'generated' not in st.session_state:\n",
    "#                 st.session_state['generated'] = []\n",
    "#             if 'past' not in st.session_state:\n",
    "#                 st.session_state['past'] = []\n",
    "#             if 'messages' not in st.session_state:\n",
    "#                 st.session_state['messages'] = [\n",
    "#                     {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "#                 ]\n",
    "\n",
    "#             response_container = st.container()\n",
    "#             # container for text box\n",
    "#             container = st.container()\n",
    "\n",
    "#             with container:\n",
    "#                 with st.form(key='my_form', clear_on_submit=True):\n",
    "#                     user_input = st.text_area(\"You:\", key='input', height=50)\n",
    "#                     submit_button = st.form_submit_button(label='Send')\n",
    "\n",
    "#                 if submit_button and user_input:\n",
    "#                     output = generate_response(user_input)\n",
    "#                     st.session_state['past'].append(user_input)\n",
    "#                     st.session_state['generated'].append(output)\n",
    "#                     #st.session_state['model_name'].append(model_name)\n",
    "\n",
    "#             if st.session_state['generated']:\n",
    "#                 with response_container:\n",
    "#                     for i in range(len(st.session_state['generated'])):\n",
    "#                         message(st.session_state[\"past\"][i], is_user=True, key=str(i) + '_user1')\n",
    "#                         message(st.session_state[\"generated\"][i], key=str(i))\n",
    "            \n",
    "            if not uploaded_file:\n",
    "                st.caption(\"Please upload research PDF documents to continue.\")\n",
    "                st.stop()\n",
    "    \n",
    "            retriever = configure_retriever(uploaded_file)\n",
    "\n",
    "            # Setup memory for contextual conversation\n",
    "            msgs = StreamlitChatMessageHistory()\n",
    "            memory = ConversationBufferMemory(memory_key=\"chat_history\", chat_memory=msgs, return_messages=True)\n",
    "\n",
    "            # Setup LLM and QA chain\n",
    "            llm = ChatOpenAI(\n",
    "                model_name=\"gpt-3.5-turbo\", openai_api_key=openai_api_key, temperature=0, streaming=True\n",
    "            )\n",
    "            qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "                llm, retriever=retriever, memory=memory, verbose=True\n",
    "            )\n",
    "\n",
    "            if len(msgs.messages) == 0 or st.sidebar.button(\"Clear message history\"):\n",
    "                msgs.clear()\n",
    "                msgs.add_ai_message(\"How can I help you?\")\n",
    "\n",
    "            avatars = {\"human\": \"user\", \"ai\": \"assistant\"}\n",
    "            for msg in msgs.messages:\n",
    "                st.chat_message(avatars[msg.type]).write(msg.content)\n",
    "\n",
    "            if user_query := st.chat_input(placeholder=\"Ask me anything!\"):\n",
    "                st.chat_message(\"user\").write(user_query)\n",
    "\n",
    "                with st.chat_message(\"assistant\"):\n",
    "                    retrieval_handler = PrintRetrievalHandler(st.container())\n",
    "                    stream_handler = StreamHandler(st.empty())\n",
    "                    response = qa_chain.run(user_query, callbacks=[retrieval_handler, stream_handler])\n",
    "                \n",
    "    \n",
    "    \n",
    "        if st.session_state.selected_tab == \"Word Analytics\":    \n",
    "            #st.subheader(f\"You have selected {selected_tab}\")\n",
    "            \n",
    "            # extract the text\n",
    "            if uploaded_file is not None:\n",
    "                pdf_reader = PdfReader(uploaded_file)\n",
    "                text = \"\"\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text()\n",
    "                \n",
    "                #Remove un-important words:\n",
    "                stop_words = set(stopwords.words('english'))\n",
    "                query_words={'participants', 'Participants' }\n",
    "                stop_words.update(query_words)\n",
    "                for word in query_words:\n",
    "                    text = text.replace(word, '')\n",
    "                    \n",
    "                # Create and generate a word cloud image:\n",
    "                wordcloud = WordCloud(stopwords=stop_words, collocations=False, max_font_size=55, max_words=25, background_color=\"black\").generate(text)\n",
    "\n",
    "                # Display the generated image:\n",
    "                plt.figure(figsize=(10,12))\n",
    "                plt.imshow(wordcloud, interpolation='bilinear')\n",
    "                plt.axis(\"off\")\n",
    "                plt.show()\n",
    "                st.set_option('deprecation.showPyplotGlobalUse', False)\n",
    "                st.pyplot()\n",
    "                \n",
    "                col1, col2 = st.columns([0.3,0.7])\n",
    "                \n",
    "                with col1:\n",
    "                    #overall doc sentiment\n",
    "                    analyzer=SentimentIntensityAnalyzer()\n",
    "                    polarity = analyzer.polarity_scores(text)['compound']\n",
    "                    st.subheader(f\"\\nOverall Research document sentiment is: {get_sentiment(polarity)}\", )\n",
    "            \n",
    "                    # splitting modified single string into list of strings using groupby() function\n",
    "                    grouped_strings = [\"\".join(g) for k, g in itertools.groupby(text, lambda x: x == \" \") if not k]\n",
    "                    \n",
    "                    #word-wise sentiment\n",
    "                    df = pd.DataFrame()\n",
    "                    df['polarity']=[analyzer.polarity_scores(text)['compound'] for text in grouped_strings]\n",
    "                    df['sentiment']=df.polarity.apply(get_sentiment)\n",
    "                    plt.figure(figsize=(3,3))\n",
    "                    df.sentiment.value_counts().plot.bar()\n",
    "                    st.pyplot()\n",
    "                    \n",
    "                    #Tokenization\n",
    "                    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "                    #compute freq distribution\n",
    "                    freq_dist = FreqDist(tokens)\n",
    "\n",
    "                    #plot the freq distribution\n",
    "                    freq_dist.plot(50, cumulative=True)\n",
    "\n",
    "                    #set labels and title\n",
    "                    # plt.xlabel('Words')\n",
    "                    # plt.ylabel('Frequency')\n",
    "                    plt.title('Frequency distribution of words')\n",
    "                    st.pyplot()\n",
    "                    \n",
    "                with col2:\n",
    "                    st.subheader(\"\\nWord frequency of the words in the research doc is: \")\n",
    "                    \n",
    "                    data2,data3,data4 = word_frequency(text)\n",
    "                    fig, axes = plt.subplots(3,1,figsize=(8,20))\n",
    "                    sns.barplot(ax=axes[0],x='frequency',y='word',data=data2.head(30))\n",
    "                    sns.barplot(ax=axes[1],x='frequency',y='pairs',data=data3.head(30))\n",
    "                    sns.barplot(ax=axes[2],x='frequency',y='trigrams',data=data4.head(30))\n",
    "                    st.pyplot(fig)\n",
    "                \n",
    "        if st.session_state.selected_tab == \"PLS Generator\":\n",
    "            if st.session_state.process_button and st.session_state.uploaded_file and st.session_state.NCT!='NCT':\n",
    "                col1, col2 = st.columns([0.2,0.8], gap=\"large\")\n",
    "                with col1:\n",
    "                    input_file = save_uploadedfile(uploaded_file)\n",
    "                    pdf_file = os.path.join(datadir, uploaded_file.name) #rootdir + \"/\" + uploaded_file.name\n",
    "                    pdf_view = displayPDF(pdf_file)\n",
    "                with col2:\n",
    "                    with st.spinner(text='Processing research document you gave on the left to generate Plain Language Summary for you...‚è≥'):\n",
    "\n",
    "                        # Progress bar\n",
    "                        #progress_bar_method(50) or st.progress(0, \"text\")\n",
    "                        \n",
    "                        # Call the processing function on the uploaded documents with user inputs\n",
    "                        replacements = process_documents(NCT, pdf_file, tense, pls_grade)\n",
    "                        st.success(\"Processed Output to be filled up in the preferred PLS template\")\n",
    "\n",
    "                        #Display processed output\n",
    "                        #st.write(replacements)\n",
    "                        st.snow()\n",
    "                        st.balloons()\n",
    "                        \n",
    "                # Store the replacements dictionary in session state\n",
    "                st.session_state.replacements = replacements\n",
    "\n",
    "            # Step 3: PPT Template Selection and Download\n",
    "            st.subheader(\"Step 3: Select PLS Template and Download\")\n",
    "            \n",
    "            default_format = \"PPT format\"\n",
    "            st.session_state.select_format = pills(\"Select PPT or Word format\", [\"PPT format\", \"Word format\"], [\"üéà\", \"üåà\"], index=[\"PPT format\", \"Word format\"].index(default_format))\n",
    "            \n",
    "            if st.session_state.select_format == \"PPT format\":\n",
    "                # Add radio buttons for template selection here    \n",
    "                default_template = \"Blue_PLS_Template.png\"\n",
    "                selected_template = image_select(\n",
    "                    label=\"Select PPT Template\",\n",
    "                    images=[\n",
    "                        os.path.join(rootdir, 'Pfizer_Blue_PLS_Template.png'),\n",
    "                        os.path.join(rootdir, 'Pfizer_Red_PLS_Template.png'),\n",
    "                        os.path.join(rootdir, 'Pfizer_Long_PLS_Template.png'),\n",
    "                    ],\n",
    "                    captions=[\"Blue_PLS_Template\", \"Red_PLS_Template\", \"Long_PLS_Template\"],\n",
    "                    index=[\"Blue_PLS_Template.png\", \"Red_PLS_Template.png\", \"Long_PLS_Template.png\"].index(default_template),\n",
    "                    use_container_width = False,\n",
    "                )\n",
    "                #selected_template = st.radio(\"Select PPT Template\", options=[\"Blue_PLS_Template\", \"Red_PLS_Template\", \"Long_PLS_Template\"], index=[\"Blue_PLS_Template\", \"Red_PLS_Template\", \"Long_PLS_Template\"].index(default_template), horizontal=True)\n",
    "                #selected_template = pills(\"\", [\"Pfizer_Blue_PLS_Template\", \"Pfizer_Red_PLS_Template\", \"Pfizer_Long_PLS_Template\"], [\"üçÄ\", \"üéà\", \"üåà\"])\n",
    "            \n",
    "            if st.session_state.select_format == \"Word format\":\n",
    "                default_template = \"Word_PLS_Template\"\n",
    "                selected_template = \"Blue_PLS_Template.png\"\n",
    "                image_select(\n",
    "                    label=\"Select Word Template\",\n",
    "                    images=[\n",
    "                        os.path.join(rootdir, 'Pfizer_Word_PLS_Template.png'),\n",
    "                    ],\n",
    "                    captions=[\"Word_PLS_Template\"],\n",
    "                    index=[\"Word_PLS_Template\"].index(default_template),\n",
    "                    use_container_width = False,\n",
    "                )\n",
    "            \n",
    "            generate_ppt_button = st.button(\"Generate PLS\")\n",
    "\n",
    "            if generate_ppt_button:\n",
    "                # Retrieve the replacements dictionary from session state\n",
    "                replacements = st.session_state.replacements\n",
    "                st.session_state.process_button = False\n",
    "\n",
    "                if replacements:\n",
    "                    with st.spinner('Generating PLS slides for you...‚è≥'):\n",
    "                        # Call the postprocessing function to generate PPT content\n",
    "                        ppt_content = postprocess_to_ppt(replacements, selected_template)\n",
    "\n",
    "                        doc_content = postprocess_to_doc(replacements)\n",
    "\n",
    "                        # Display the PPT content using st.markdown or st.write\n",
    "                        #st.markdown(ppt_content, unsafe_allow_html=True)\n",
    "                        st.markdown(list(replacements.keys()))\n",
    "\n",
    "                        # Store the PPT content in session state\n",
    "                        st.session_state.ppt_content = ppt_content\n",
    "                        st.session_state.doc_content = doc_content\n",
    "\n",
    "                         # Step 4: PPT Download\n",
    "                        if \"ppt_content\" and \"doc_content\" in st.session_state:\n",
    "                            ppt_content = st.session_state.ppt_content\n",
    "                            doc_content = st.session_state.doc_content\n",
    "\n",
    "                            st.session_state.replacements = replacements\n",
    "                            st.session_state.process_button = False\n",
    "\n",
    "                            # Save the modified presentation object to a temporary file\n",
    "                            #ppt_output_file = f\"PLS_{replacements['<Title>']}_{datetime.now().strftime('%Y%m%d%H%M%S')}.pptx\"                    \n",
    "                            ppt_output_file = \"PLS_PPT.pptx\"\n",
    "                            #ppt_content.save(ppt_output_file)\n",
    "\n",
    "                            # save presentation as binary output\n",
    "                            binary_output = BytesIO()\n",
    "                            ppt_content.save(binary_output)\n",
    "\n",
    "                            binary_output_doc = BytesIO()\n",
    "                            doc_content.save(binary_output_doc)\n",
    "\n",
    "                            # display success message and download button\n",
    "                            st.success(':tada: The PLS template has been filled with above sections in ' + selected_template)\n",
    "\n",
    "                            # Provide the download link for the generated PPT and DOC\n",
    "                            if st.session_state.select_format == \"PPT format\":\n",
    "                                st.download_button(\"Download PLS PPT\", data=binary_output.getvalue(), file_name=ppt_output_file, mime=\"application/vnd.openxmlformats-officedocument.presentationml.presentation\")\n",
    "                            if st.session_state.select_format == \"Word format\":\n",
    "                                st.download_button(\"Download PLS Doc\", data=binary_output_doc.getvalue(), file_name=\"PLS_DOC.docx\", mime=\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\")\n",
    "\n",
    "                                \n",
    "        if st.session_state.selected_tab == \"Search and Chat with PubMed/CTGov\":\n",
    "            st.subheader(\"Search PubMed and ClinicalTrials.gov\")\n",
    "            query = st.text_input(\"Enter your query:\")\n",
    "            search_option = st.radio(\"Search Option\", [\"Search PubMed\", \"Search ClinicalTrials.gov\", \"Search Both\"], horizontal=True)\n",
    "\n",
    "            col1, col2 = st.columns(2)  # Split the main container into two columns\n",
    "            if query:\n",
    "                if search_option == \"Search PubMed\":\n",
    "                    pubmed_articles = search_pubmed(query)[:5]\n",
    "                    st.session_state.articles = pubmed_articles\n",
    "                    with col1:\n",
    "                        st.write(\"### PubMed Results\")\n",
    "                        display_articles(pubmed_articles)\n",
    "                    # Create a DataFrame in the second column with PubMed articles\n",
    "                    with col2:\n",
    "                        df_pubmed = pd.DataFrame(pubmed_articles)\n",
    "                        st.write(\"### Converse with retrieved PubMed Articles\")\n",
    "                        st.write(df_pubmed)\n",
    "\n",
    "                elif search_option == \"Search ClinicalTrials.gov\":\n",
    "                    ctgov_articles = search_ctgov(query)[:5]\n",
    "                    st.session_state.articles = ctgov_articles\n",
    "                    with col1:\n",
    "                        st.write(\"### ClinicalTrials.gov Results\")\n",
    "                        display_articles(ctgov_articles)\n",
    "                    # Create a DataFrame in the second column with ClinicalTrials.gov articles\n",
    "                    with col2:\n",
    "                        df_ctgov = pd.DataFrame(ctgov_articles)\n",
    "                        st.write(\"### Converse with retrieved CTGov Articles\")\n",
    "                        st.write(df_ctgov)\n",
    "\n",
    "                else:  # Search Both\n",
    "                    pubmed_articles = search_pubmed(query)[:5]\n",
    "                    ctgov_articles = search_ctgov(query)[:5]\n",
    "                    st.session_state.articles = pubmed_articles + ctgov_articles\n",
    "                    with col1:\n",
    "                        st.write(\"### PubMed Results\")\n",
    "                        display_articles(pubmed_articles)\n",
    "                    with col1:  # Use the same column for ClinicalTrials.gov Results\n",
    "                        st.write(\"### ClinicalTrials.gov Results\")\n",
    "                        display_articles(ctgov_articles)\n",
    "                    # Concatenate both sets of articles and create a DataFrame in the second column\n",
    "                    with col2:\n",
    "                        combined_articles = pubmed_articles + ctgov_articles\n",
    "                        df_combined = pd.DataFrame(combined_articles)\n",
    "                        st.write(\"### Converse with retrieved PubMed/CTGov Articles\")\n",
    "                        st.write(df_combined)\n",
    "            # Placeholder for chatbot implementation in the second column\n",
    "            with col2:\n",
    "                st.write(\"Chatbot - Ask questions from only these Pubmed/CTGov articles\")\n",
    "                if 'articles' in st.session_state:      \n",
    "                    df = pd.DataFrame(st.session_state.articles)\n",
    "                    #user_prompt = st.text_area(label=\"prompt:\",placeholder=\"Number of patients..\",)\n",
    "                    #if st.button(\"Generate\"):\n",
    "                ########################################################LangChain CSV Agent (with Pandas)\n",
    "\n",
    "                    # langchain_pandas_agent = create_pandas_dataframe_agent(\n",
    "                    #     ChatOpenAI(temperature=0, model=\"gpt-4-32k\", streaming=True, ),\n",
    "                    #     df,\n",
    "                    #     verbose=True,\n",
    "                    #     agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    "                    # )\n",
    "\n",
    "                    #st.write(\"Langchain pandas agent: \", langchain_pandas_agent.run(user_prompt))\n",
    "                #####################################for only QnA, not chatbot########use code in this brackets\n",
    "#                     # Initialise session state variables\n",
    "#                     if 'generated1' not in st.session_state:\n",
    "#                         st.session_state['generated1'] = []\n",
    "#                     if 'past1' not in st.session_state:\n",
    "#                         st.session_state['past1'] = []\n",
    "#                     if 'messages1' not in st.session_state:\n",
    "#                         st.session_state['messages1'] = [\n",
    "#                             {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "#                         ]\n",
    "\n",
    "\n",
    "#                     # container for chat history\n",
    "#                     response_container = st.container()\n",
    "\n",
    "#                     # container for text box\n",
    "#                     input_container = st.container()\n",
    "\n",
    "#                     with input_container:\n",
    "#                         # Create a form for user input\n",
    "#                         with st.form(key='my_form', clear_on_submit=True):\n",
    "#                             user_input = st.text_area(\"You:\", key='input', height=100)\n",
    "#                             submit_button = st.form_submit_button(label='Send')\n",
    "\n",
    "#                         if submit_button and user_input:\n",
    "#                             # If user submits input, generate response and store input and response in session state variables\n",
    "#                             try:\n",
    "#                                 query_response = generate_response1(user_input, df)\n",
    "#                                 st.session_state['past1'].append(user_input)\n",
    "#                                 st.session_state['generated1'].append(query_response)\n",
    "#                             except Exception as e:\n",
    "#                                 st.error(\"An error occurred: {}\".format(e))\n",
    "\n",
    "#                     if st.session_state['generated1']:\n",
    "#                         # Display chat history in a container\n",
    "#                         with response_container:\n",
    "#                             for i in range(len(st.session_state['generated1'])):\n",
    "#                                 message(st.session_state[\"past1\"][i], is_user=True, key=str(i) + '_user')\n",
    "#                                 message(st.session_state[\"generated1\"][i], key=str(i))\n",
    "                        \n",
    "        \n",
    "                    if \"messages\" not in st.session_state or st.sidebar.button(\"Clear conversation history\"):\n",
    "                        st.session_state[\"messages\"] = [{\"role\": \"assistant\", \"content\": \"How can I help you?\"}]\n",
    "\n",
    "                    for msg in st.session_state.messages:\n",
    "                        st.chat_message(msg[\"role\"]).write(msg[\"content\"])\n",
    "\n",
    "                    if prompt := st.text_input(\"You:\",placeholder=\"What is this data about?\"):\n",
    "                        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "                        st.chat_message(\"user\").write(prompt)\n",
    "\n",
    "                        llm = ChatOpenAI(\n",
    "                            temperature=0, model=\"gpt-3.5-turbo-0613\", openai_api_key=openai_api_key, streaming=True\n",
    "                        )\n",
    "\n",
    "                        pandas_df_agent = create_pandas_dataframe_agent(\n",
    "                            llm,\n",
    "                            df,\n",
    "                            verbose=True,\n",
    "                            agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    "                            handle_parsing_errors=True,\n",
    "                        )\n",
    "\n",
    "                        with st.chat_message(\"assistant\"):\n",
    "                            st_cb = StreamlitCallbackHandler(st.container(), expand_new_thoughts=False)\n",
    "                            response = pandas_df_agent.run(st.session_state.messages, callbacks=[st_cb])\n",
    "                            st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "                            st.write(response)\n",
    "                            \n",
    "                            \n",
    "                        # Add a download button for the chat conversation\n",
    "                        #if st.button(\"Download Chat Conversation\"):\n",
    "                            #download_chat_conversation(st.session_state['past'], st.session_state['generated'])\n",
    "                                \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13de42d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
